---
name: content-diversifier
description: "å†…å®¹å¤šæ ·åŒ–æ”¹å†™å¼•æ“ - é€šè¿‡åŒä¹‰æ›¿æ¢ã€å¥å¼é‡ç»„ã€æ®µè½é‡æ„ç­‰ç­–ç•¥é™ä½æ–‡æœ¬æŸ¥é‡ç‡,åŒæ—¶ä¿æŒä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§ã€‚é€‚ç”¨äºç”Ÿæˆå¤šä¸ªç‰ˆæœ¬çš„æŠ¥å‘Šã€æ–¹æ¡ˆç­‰æ–‡æ¡£ã€‚"
---

# Content Diversifier - å†…å®¹å¤šæ ·åŒ–æ”¹å†™å¼•æ“

## æŠ€èƒ½è¯´æ˜

è¿™æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬å¤šæ ·åŒ–æ”¹å†™å¼•æ“,ä¸“é—¨ç”¨äºé™ä½æ–‡æ¡£æŸ¥é‡ç‡,åŒæ—¶ä¿æŒå†…å®¹çš„ä¸“ä¸šæ€§ã€å‡†ç¡®æ€§å’Œå¯è¯»æ€§ã€‚

**æ ¸å¿ƒèƒ½åŠ›**:
- ğŸ”„ åŒä¹‰è¯æ™ºèƒ½æ›¿æ¢
- ğŸ”€ å¥å¼ç»“æ„é‡ç»„
- ğŸ“ æ®µè½é€»è¾‘é‡æ„
- ğŸ¯ æ•°å€¼è¡¨è¾¾å¤šæ ·åŒ–
- ğŸ”’ å…³é”®æœ¯è¯­ä¿æŠ¤

**åº”ç”¨åœºæ™¯**:
- æ™ºèƒ½å»ºé€ å®æ–½æ–¹æ¡ˆç”Ÿæˆ
- æŠ€æœ¯æ–¹æ¡ˆæ–‡æ¡£ç¼–åˆ¶
- ç”³æŠ¥èµ„æ–™å‡†å¤‡
- æ ‡å‡†åŒ–æŠ¥å‘Šåˆ¶ä½œ

**ç›®æ ‡æ•ˆæœ**:
- æŸ¥é‡ç‡é™ä½è‡³ 15-20%
- ä¿æŒä¸“ä¸šæœ¯è¯­å‡†ç¡®æ€§
- ä¿æŒæ•°å€¼æ•°æ®ä¸€è‡´æ€§
- ä¿æŒé€»è¾‘è¿è´¯æ€§

---

## è¾“å…¥å‚æ•°

| å‚æ•°å | ç±»å‹ | å¿…éœ€ | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|------|------|--------|------|
| `input_text` | string | âœ… | - | éœ€è¦æ”¹å†™çš„åŸå§‹æ–‡æœ¬ |
| `diversification_level` | string | âš ï¸ | "medium" | æ”¹å†™å¼ºåº¦: low/medium/high |
| `preserve_data` | boolean | âš ï¸ | true | æ˜¯å¦ä¿ç•™æ•°å€¼æ•°æ® |
| `preserve_terms` | array | âš ï¸ | [] | éœ€è¦ä¿ç•™çš„ä¸“ä¸šæœ¯è¯­åˆ—è¡¨ |
| `target_similarity` | float | âš ï¸ | 0.2 | ç›®æ ‡ç›¸ä¼¼åº¦(0-1,è¶Šå°è¶Šä¸ç›¸ä¼¼) |
| `preserve_structure` | boolean | âš ï¸ | true | æ˜¯å¦ä¿ç•™æ®µè½ç»“æ„ |

---

## æ‰§è¡Œé€»è¾‘

### é˜¶æ®µ 1: æ–‡æœ¬é¢„å¤„ç†ä¸åˆ†æ

#### æ­¥éª¤ 1.1: æ–‡æœ¬ç»“æ„åˆ†æ

```python
def analyze_text_structure(text):
    """åˆ†ææ–‡æœ¬ç»“æ„"""

    structure = {
        "paragraphs": [],        # æ®µè½åˆ—è¡¨
        "sentences": [],         # å¥å­åˆ—è¡¨
        "data_patterns": [],     # æ•°å€¼æ¨¡å¼
        "term_patterns": [],     # æœ¯è¯­æ¨¡å¼
        "structure_type": ""     # ç»“æ„ç±»å‹
    }

    # åˆ†å‰²æ®µè½
    paragraphs = text.split('\n\n')
    for para in paragraphs:
        if para.strip():
            structure["paragraphs"].append({
                "text": para.strip(),
                "sentence_count": para.count('ã€‚') + para.count('!') + para.count('?'),
                "char_count": len(para)
            })

    # æå–å¥å­
    import re
    sentences = re.split(r'[ã€‚!?]', text)
    structure["sentences"] = [s.strip() for s in sentences if s.strip()]

    # è¯†åˆ«æ•°å€¼æ¨¡å¼
    data_patterns = re.findall(r'\d+(?:\.\d+)?(?:ä¸‡|äº¿|åƒ|ç™¾)?(?:å…ƒ|ç±³|å¹³æ–¹ç±³|ã¡|ä¸ª|äºº|å¤©|æœˆ|å¹´)?', text)
    structure["data_patterns"] = list(set(data_patterns))

    # è¯†åˆ«ç»“æ„ç±»å‹
    if text.count('\n\n') > 5:
        structure["structure_type"] = "multi_paragraph"
    elif any(keyword in text for keyword in ['ç¬¬ä¸€', 'ç¬¬äºŒ', 'ç¬¬ä¸‰', '1.', '2.', '3.']):
        structure["structure_type"] = "enumerated"
    else:
        structure["structure_type"] = "narrative"

    return structure
```

**è¾“å‡ºç¤ºä¾‹**:
```json
{
  "paragraphs": [
    {
      "text": "æœ¬é¡¹ç›®ä½äºè‹å·å¸‚å·¥ä¸šå›­åŒº,æ€»å»ºç­‘é¢ç§¯50000å¹³æ–¹ç±³ã€‚",
      "sentence_count": 1,
      "char_count": 28
    }
  ],
  "sentences": ["æœ¬é¡¹ç›®ä½äºè‹å·å¸‚å·¥ä¸šå›­åŒº", "æ€»å»ºç­‘é¢ç§¯50000å¹³æ–¹ç±³"],
  "data_patterns": ["50000å¹³æ–¹ç±³", "è‹å·å¸‚"],
  "structure_type": "narrative"
}
```

#### æ­¥éª¤ 1.2: æå–éœ€è¦ä¿æŠ¤çš„å…ƒç´ 

```python
def extract_protected_elements(text, preserve_terms, preserve_data):
    """æå–éœ€è¦ä¿æŠ¤çš„å…ƒç´ """
    import re

    protected = {
        "terms": [],     # ä¸“ä¸šæœ¯è¯­
        "data": [],      # æ•°å€¼æ•°æ®
        "names": [],     # ä¸“æœ‰åè¯
        "positions": {}  # ä½ç½®æ˜ å°„
    }

    # ä¿æŠ¤ä¸“ä¸šæœ¯è¯­
    for term in preserve_terms:
        positions = [m.start() for m in re.finditer(re.escape(term), text)]
        if positions:
            protected["terms"].append({
                "term": term,
                "count": len(positions),
                "positions": positions
            })

    # ä¿æŠ¤æ•°å€¼æ•°æ®
    if preserve_data:
        data_pattern = r'\d+(?:\.\d+)?(?:ä¸‡|äº¿|åƒ|ç™¾)?(?:å…ƒ|ç±³|å¹³æ–¹ç±³|ã¡|mÂ²|ä¸ª|äºº|å¤©|æœˆ|å¹´|%|â„ƒ)?'
        for match in re.finditer(data_pattern, text):
            protected["data"].append({
                "value": match.group(),
                "start": match.start(),
                "end": match.end()
            })

    # è¯†åˆ«ä¸“æœ‰åè¯(åœ°åã€å•ä½åç­‰)
    # ç®€åŒ–å¤„ç†:åŒ…å«"å¸‚"ã€"çœ"ã€"å…¬å¸"ã€"é›†å›¢"ç­‰çš„è¯ç»„
    name_pattern = r'[\u4e00-\u9fa5]{2,}(?:å¸‚|çœ|å¿|åŒº|å…¬å¸|é›†å›¢|æœ‰é™å…¬å¸|è‚¡ä»½å…¬å¸)'
    for match in re.finditer(name_pattern, text):
        protected["names"].append({
            "name": match.group(),
            "start": match.start(),
            "end": match.end()
        })

    return protected
```

---

### é˜¶æ®µ 2: ç­–ç•¥ 1 - åŒä¹‰è¯æ›¿æ¢

#### æ­¥éª¤ 2.1: æ„å»ºåŒä¹‰è¯è¯å…¸

å»ºç«‹å»ºç­‘è¡Œä¸šä¸“ä¸šçš„åŒä¹‰è¯è¯å…¸:

```python
# å»ºç­‘è¡Œä¸šåŒä¹‰è¯è¯å…¸
SYNONYM_DICT = {
    # åŠ¨è¯
    "é‡‡ç”¨": ["è¿ç”¨", "åº”ç”¨", "ä½¿ç”¨", "é€‰ç”¨"],
    "è¿›è¡Œ": ["å¼€å±•", "å®æ–½", "æ‰§è¡Œ", "æ¨è¿›"],
    "å»ºè®¾": ["å»ºé€ ", "æ–½å·¥", "å»ºç«‹", "æ„å»º"],
    "æé«˜": ["æå‡", "å¢å¼º", "æ”¹å–„", "ä¼˜åŒ–"],
    "åŠ å¼º": ["å¼ºåŒ–", "å¢å¼º", "å·©å›º", "æ·±åŒ–"],
    "å®Œå–„": ["å¥å…¨", "ä¼˜åŒ–", "æ”¹è¿›", "æå‡"],
    "æ¨è¿›": ["æ¨åŠ¨", "ä¿ƒè¿›", "åŠ å¿«", "æ·±åŒ–"],
    "å®ç°": ["è¾¾åˆ°", "å®Œæˆ", "è¾¾æˆ", "åšåˆ°"],

    # åè¯
    "é¡¹ç›®": ["å·¥ç¨‹", "é¡¹ç›®å·¥ç¨‹", "å»ºè®¾é¡¹ç›®", "å·¥ç¨‹é¡¹ç›®"],
    "æ–¹æ¡ˆ": ["è®¡åˆ’", "è§„åˆ’", "ç­–åˆ’", "è®¾è®¡æ–¹æ¡ˆ"],
    "æŠ€æœ¯": ["å·¥è‰º", "æŠ€æœ¯æ‰‹æ®µ", "æŠ€æœ¯æ–¹æ³•", "æŠ€æœ¯æªæ–½"],
    "ç®¡ç†": ["ç®¡æ§", "ç®¡ç†å·¥ä½œ", "ç®¡ç†æªæ–½", "ç®¡ç†ä½“ç³»"],
    "è´¨é‡": ["å·¥ç¨‹è´¨é‡", "è´¨é‡æ°´å¹³", "å“è´¨"],
    "å®‰å…¨": ["å®‰å…¨ç®¡ç†", "å®‰å…¨å·¥ä½œ", "å®‰å…¨ä¿éšœ"],
    "æ•ˆç‡": ["æ•ˆèƒ½", "å·¥ä½œæ•ˆç‡", "ç”Ÿäº§æ•ˆç‡"],
    "ç›®æ ‡": ["ç›®çš„", "å®—æ—¨", "ç›®æ ‡å€¼", "é¢„æœŸç›®æ ‡"],
    "æªæ–½": ["åŠæ³•", "æ–¹æ³•", "ä¸¾æª", "å¯¹ç­–"],
    "åˆ¶åº¦": ["ä½“ç³»", "æœºåˆ¶", "è§„ç« ", "ç®¡ç†åˆ¶åº¦"],

    # å½¢å®¹è¯
    "å…ˆè¿›": ["é¢†å…ˆ", "å‰æ²¿", "ç°ä»£åŒ–", "é«˜æ°´å¹³"],
    "å®Œå–„": ["å¥å…¨", "å®Œå¤‡", "å…¨é¢", "ç³»ç»Ÿ"],
    "é‡è¦": ["å…³é”®", "æ ¸å¿ƒ", "ä¸»è¦", "é‡ç‚¹"],
    "æœ‰æ•ˆ": ["é«˜æ•ˆ", "åˆ‡å®", "å®ç”¨", "å¯è¡Œ"],
    "å…¨é¢": ["ç»¼åˆ", "ç³»ç»Ÿ", "æ•´ä½“", "å®Œæ•´"],
    "ç§‘å­¦": ["åˆç†", "è§„èŒƒ", "ç³»ç»Ÿ", "ä¸“ä¸š"],

    # çŸ­è¯­
    "æœ‰åˆ©äº": ["ä¾¿äº", "åˆ©äº", "æœ‰åŠ©äº", "ä¿ƒè¿›"],
    "ç¡®ä¿": ["ä¿è¯", "ç¡®è®¤", "ä¿éšœ", "ç»´æŠ¤"],
    "é€šè¿‡": ["ç»è¿‡", "å€ŸåŠ©", "ä¾é ", "å‡­å€Ÿ"],
    "æ ¹æ®": ["æŒ‰ç…§", "ä¾æ®", "åŸºäº", "éµå¾ª"],
}

# BIMå’Œæ™ºèƒ½å»ºé€ ç›¸å…³æœ¯è¯­(éœ€è¦ä¿æŠ¤,ä¸æ›¿æ¢)
PROTECTED_TERMS = [
    "BIM", "å»ºç­‘ä¿¡æ¯æ¨¡å‹",
    "æ™ºæ…§å·¥åœ°", "æ™ºèƒ½å»ºé€ ",
    "è£…é…å¼å»ºç­‘", "ç»¿è‰²å»ºç­‘",
    "è´¨é‡éªŒæ”¶", "å®‰å…¨ç”Ÿäº§",
    "æ··å‡åœŸ", "é’¢ç­‹", "æ¨¡æ¿",
    "GB", "JGJ", "è§„èŒƒ", "æ ‡å‡†"
]
```

#### æ­¥éª¤ 2.2: æ‰§è¡ŒåŒä¹‰è¯æ›¿æ¢

```python
def apply_synonym_replacement(text, protected_elements, diversification_level):
    """åº”ç”¨åŒä¹‰è¯æ›¿æ¢"""
    import re
    import random

    # æ ¹æ®æ”¹å†™å¼ºåº¦è®¾ç½®æ›¿æ¢æ¯”ä¾‹
    replacement_ratios = {
        "low": 0.3,      # 30%çš„è¯æ›¿æ¢
        "medium": 0.5,   # 50%çš„è¯æ›¿æ¢
        "high": 0.7      # 70%çš„è¯æ›¿æ¢
    }
    ratio = replacement_ratios.get(diversification_level, 0.5)

    result_text = text
    replacements = []

    # éå†åŒä¹‰è¯è¯å…¸
    for original, synonyms in SYNONYM_DICT.items():
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿æŠ¤
        if original in [p["term"] for p in protected_elements["terms"]]:
            continue

        # æŸ¥æ‰¾æ‰€æœ‰å‡ºç°ä½ç½®
        positions = [m.start() for m in re.finditer(re.escape(original), result_text)]

        # éšæœºå†³å®šæ˜¯å¦æ›¿æ¢(æ ¹æ®ratio)
        for pos in positions:
            if random.random() < ratio:
                # éšæœºé€‰æ‹©ä¸€ä¸ªåŒä¹‰è¯
                synonym = random.choice(synonyms)

                # æ‰§è¡Œæ›¿æ¢(ä»åå‘å‰æ›¿æ¢,é¿å…ä½ç½®åç§»)
                result_text = (
                    result_text[:pos] +
                    synonym +
                    result_text[pos + len(original):]
                )

                replacements.append({
                    "original": original,
                    "synonym": synonym,
                    "position": pos
                })

    return result_text, replacements
```

**ç¤ºä¾‹æ•ˆæœ**:
```
åŸæ–‡: "æœ¬é¡¹ç›®é‡‡ç”¨BIMæŠ€æœ¯è¿›è¡Œä¸‰ç»´å»ºæ¨¡,æœ‰æ•ˆæé«˜æ–½å·¥æ•ˆç‡ã€‚"
æ”¹å†™: "æœ¬å·¥ç¨‹è¿ç”¨BIMæŠ€æœ¯å¼€å±•ç«‹ä½“åŒ–å»ºæ¨¡,æœ‰æ•ˆæå‡æ–½å·¥æ•ˆèƒ½ã€‚"

æ›¿æ¢è®°å½•:
- "é¡¹ç›®" â†’ "å·¥ç¨‹" (ä½ç½®:2)
- "é‡‡ç”¨" â†’ "è¿ç”¨" (ä½ç½®:5)
- "è¿›è¡Œ" â†’ "å¼€å±•" (ä½ç½®:15)
- "æé«˜" â†’ "æå‡" (ä½ç½®:25)
- "æ•ˆç‡" â†’ "æ•ˆèƒ½" (ä½ç½®:30)

ä¿æŠ¤çš„æœ¯è¯­:
- "BIMæŠ€æœ¯" (ä¸“ä¸šæœ¯è¯­,æœªæ›¿æ¢)
```

---

### é˜¶æ®µ 3: ç­–ç•¥ 2 - å¥å¼é‡ç»„

#### æ­¥éª¤ 3.1: è¯†åˆ«å¥å­ç»“æ„

```python
def identify_sentence_structure(sentence):
    """è¯†åˆ«å¥å­ç»“æ„"""

    # ç®€åŒ–çš„å¥å­ç»“æ„åˆ†ç±»
    if ',' in sentence or 'ã€' in sentence:
        return "complex"  # å¤å¥
    elif len(sentence) < 15:
        return "simple"   # ç®€å•å¥
    else:
        return "compound" # å¹¶åˆ—å¥
```

#### æ­¥éª¤ 3.2: æ‰§è¡Œå¥å¼é‡ç»„

```python
def restructure_sentence(sentence, structure_type):
    """é‡ç»„å¥å­ç»“æ„"""
    import re
    import random

    restructured = sentence

    if structure_type == "complex":
        # å¤å¥:å°è¯•æ‹†åˆ†æˆ–è°ƒæ•´é¡ºåº
        parts = re.split(r'[,,ã€]', sentence)

        if len(parts) >= 2 and random.random() > 0.5:
            # ç­–ç•¥1: æ‹†åˆ†æˆå¤šä¸ªçŸ­å¥
            restructured = 'ã€‚'.join([p.strip() for p in parts if p.strip()]) + 'ã€‚'
        else:
            # ç­–ç•¥2: è°ƒæ•´é¡ºåº
            if len(parts) >= 3:
                random.shuffle(parts)
                restructured = 'ã€'.join(parts)

    elif structure_type == "simple":
        # ç®€å•å¥:å¯ä»¥åˆå¹¶æˆ–æ‰©å±•
        # æš‚æ—¶ä¿æŒåŸæ ·
        pass

    elif structure_type == "compound":
        # å¹¶åˆ—å¥:å°è¯•å€’è£…æˆ–é‡ç»„
        # ä¾‹å¦‚: "Aå»ºç­‘é¢ç§¯B,Cå·¥æœŸD" â†’ "è®¡åˆ’å·¥æœŸDçš„é¡¹ç›®C,å»ºç­‘é¢ç§¯B"

        # è¯†åˆ«ä¸»è¯­ã€è°“è¯­ã€å®¾è¯­(ç®€åŒ–å¤„ç†)
        if 'ä½äº' in sentence and 'å»ºç­‘é¢ç§¯' in sentence:
            # æå–åœ°ç‚¹å’Œé¢ç§¯
            location_match = re.search(r'ä½äº(.+?)[,,,]', sentence)
            area_match = re.search(r'å»ºç­‘é¢ç§¯(.+?)(?:[,ã€‚]|$)', sentence)

            if location_match and area_match:
                location = location_match.group(1).strip()
                area = area_match.group(1).strip()

                # é‡ç»„
                restructured = f"å»ºç­‘é¢ç§¯{area}çš„æœ¬é¡¹ç›®,ä½äº{location}ã€‚"

    return restructured
```

**ç¤ºä¾‹æ•ˆæœ**:
```
åŸæ–‡: "æœ¬é¡¹ç›®ä½äºè‹å·å¸‚å·¥ä¸šå›­åŒº,æ€»å»ºç­‘é¢ç§¯50000å¹³æ–¹ç±³,å»ºè®¾å·¥æœŸ24ä¸ªæœˆã€‚"

æ”¹å†™æ–¹å¼A(æ‹†åˆ†):
"æœ¬é¡¹ç›®ä½äºè‹å·å¸‚å·¥ä¸šå›­åŒºã€‚æ€»å»ºç­‘é¢ç§¯50000å¹³æ–¹ç±³ã€‚å»ºè®¾å·¥æœŸ24ä¸ªæœˆã€‚"

æ”¹å†™æ–¹å¼B(å€’è£…):
"å»ºè®¾å·¥æœŸ24ä¸ªæœˆçš„æœ¬é¡¹ç›®,æ€»å»ºç­‘é¢ç§¯50000å¹³æ–¹ç±³,ä½äºè‹å·å¸‚å·¥ä¸šå›­åŒºã€‚"

æ”¹å†™æ–¹å¼C(åˆå¹¶éƒ¨åˆ†):
"ä½äºè‹å·å¸‚å·¥ä¸šå›­åŒºçš„æœ¬é¡¹ç›®,å»ºç­‘é¢ç§¯50000å¹³æ–¹ç±³,è®¡åˆ’å·¥æœŸ24ä¸ªæœˆã€‚"
```

---

### é˜¶æ®µ 4: ç­–ç•¥ 3 - æ®µè½é‡æ„

#### æ­¥éª¤ 4.1: åˆ†ææ®µè½é€»è¾‘å…³ç³»

```python
def analyze_paragraph_logic(paragraph):
    """åˆ†ææ®µè½é€»è¾‘å…³ç³»"""

    sentences = paragraph.split('ã€‚')
    sentences = [s.strip() for s in sentences if s.strip()]

    # è¯†åˆ«é€»è¾‘å…³ç³»è¯
    logic_markers = {
        "å› æœ": ["å› æ­¤", "æ‰€ä»¥", "å› è€Œ", "æ•…è€Œ"],
        "è½¬æŠ˜": ["ä½†æ˜¯", "ç„¶è€Œ", "ä¸è¿‡", "å¯æ˜¯"],
        "é€’è¿›": ["è€Œä¸”", "å¹¶ä¸”", "åŒæ—¶", "æ­¤å¤–"],
        "é¡ºåº": ["é¦–å…ˆ", "å…¶æ¬¡", "ç„¶å", "æœ€å"]
    }

    relations = []
    for i, sent in enumerate(sentences):
        relation = "å¹¶åˆ—"  # é»˜è®¤å¹¶åˆ—å…³ç³»

        for rel_type, markers in logic_markers.items():
            if any(marker in sent for marker in markers):
                relation = rel_type
                break

        relations.append({
            "index": i,
            "sentence": sent,
            "relation": relation
        })

    return relations
```

#### æ­¥éª¤ 4.2: é‡æ„æ®µè½ç»“æ„

```python
def reconstruct_paragraph(paragraph, preserve_structure):
    """é‡æ„æ®µè½ç»“æ„"""
    import random

    if preserve_structure:
        # ä¿ç•™ç»“æ„,åªè°ƒæ•´å¥å­å†…éƒ¨
        return paragraph

    # åˆ†æé€»è¾‘å…³ç³»
    relations = analyze_paragraph_logic(paragraph)

    if len(relations) < 2:
        return paragraph  # å•å¥æ®µè½,ä¸é‡æ„

    # è¯†åˆ«å…³é”®å¥(åŒ…å«å› æœã€è½¬æŠ˜å…³ç³»çš„å¥å­)
    key_sentences = [r for r in relations if r["relation"] in ["å› æœ", "è½¬æŠ˜"]]
    normal_sentences = [r for r in relations if r["relation"] == "å¹¶åˆ—"]

    # é‡æ„ç­–ç•¥
    if len(normal_sentences) >= 2 and random.random() > 0.5:
        # ç­–ç•¥1: è°ƒæ•´å¹¶åˆ—å¥é¡ºåº
        random.shuffle(normal_sentences)

        # é‡æ–°ç»„åˆ
        reconstructed = []
        for sent in normal_sentences:
            reconstructed.append(sent["sentence"])
        for sent in key_sentences:
            reconstructed.append(sent["sentence"])

        return 'ã€‚'.join(reconstructed) + 'ã€‚'
    else:
        # ç­–ç•¥2: åˆå¹¶ç›¸å…³å¥å­
        if len(relations) >= 3:
            # åˆå¹¶å‰ä¸¤å¥
            merged_first = relations[0]["sentence"] + ',' + relations[1]["sentence"]
            remaining = [r["sentence"] for r in relations[2:]]

            return merged_first + 'ã€‚' + 'ã€‚'.join(remaining) + 'ã€‚'

    return paragraph
```

**ç¤ºä¾‹æ•ˆæœ**:
```
åŸæ®µè½:
"æœ¬é¡¹ç›®é‡‡ç”¨BIMæŠ€æœ¯è¿›è¡Œå»ºæ¨¡ã€‚é€šè¿‡ä¸‰ç»´å¯è§†åŒ–æé«˜è®¾è®¡è´¨é‡ã€‚å®ç°æ–½å·¥è¿‡ç¨‹çš„æ•°å­—åŒ–ç®¡ç†ã€‚æœ‰æ•ˆé™ä½æˆæœ¬å’Œå·¥æœŸã€‚"

é‡æ„æ–¹å¼A(è°ƒæ•´é¡ºåº):
"å®ç°æ–½å·¥è¿‡ç¨‹çš„æ•°å­—åŒ–ç®¡ç†ã€‚é€šè¿‡ä¸‰ç»´å¯è§†åŒ–æé«˜è®¾è®¡è´¨é‡ã€‚æœ¬é¡¹ç›®é‡‡ç”¨BIMæŠ€æœ¯è¿›è¡Œå»ºæ¨¡ã€‚æœ‰æ•ˆé™ä½æˆæœ¬å’Œå·¥æœŸã€‚"

é‡æ„æ–¹å¼B(åˆå¹¶å¥å­):
"æœ¬é¡¹ç›®é‡‡ç”¨BIMæŠ€æœ¯è¿›è¡Œå»ºæ¨¡,é€šè¿‡ä¸‰ç»´å¯è§†åŒ–æé«˜è®¾è®¡è´¨é‡ã€‚å®ç°æ–½å·¥è¿‡ç¨‹çš„æ•°å­—åŒ–ç®¡ç†ã€‚æœ‰æ•ˆé™ä½æˆæœ¬å’Œå·¥æœŸã€‚"

é‡æ„æ–¹å¼C(æ‹†åˆ†æ‰©å±•):
"æœ¬é¡¹ç›®é‡‡ç”¨BIMæŠ€æœ¯ã€‚BIMæŠ€æœ¯é€šè¿‡ä¸‰ç»´å¯è§†åŒ–æ‰‹æ®µ,æœ‰æ•ˆæé«˜äº†è®¾è®¡è´¨é‡,å®ç°äº†æ–½å·¥è¿‡ç¨‹çš„æ•°å­—åŒ–ç®¡ç†,ä»è€Œé™ä½é¡¹ç›®æˆæœ¬å’Œç¼©çŸ­å·¥æœŸã€‚"
```

---

### é˜¶æ®µ 5: ç­–ç•¥ 4 - æ•°å€¼è¡¨è¾¾å¤šæ ·åŒ–

#### æ­¥éª¤ 5.1: è¯†åˆ«æ•°å€¼è¡¨è¾¾

```python
def identify_numerical_expressions(text):
    """è¯†åˆ«æ•°å€¼è¡¨è¾¾"""
    import re

    patterns = {
        "area": r'(\d+(?:\.\d+)?)(?:å¹³æ–¹ç±³|ã¡|mÂ²)',
        "money": r'(\d+(?:\.\d+)?)(?:ä¸‡å…ƒ|äº¿å…ƒ|å…ƒ)',
        "time": r'(\d+)(?:ä¸ªæœˆ|æœˆ|å¹´|å¤©)',
        "quantity": r'(\d+)(?:ä¸ª|å°|äºº|é¡¹)',
        "percentage": r'(\d+(?:\.\d+)?)%',
    }

    expressions = []
    for expr_type, pattern in patterns.items():
        for match in re.finditer(pattern, text):
            expressions.append({
                "type": expr_type,
                "original": match.group(),
                "value": match.group(1),
                "unit": match.group().replace(match.group(1), ''),
                "start": match.start(),
                "end": match.end()
            })

    return expressions
```

#### æ­¥éª¤ 5.2: å¤šæ ·åŒ–æ•°å€¼è¡¨è¾¾

```python
def diversify_numerical_expression(expression, preserve_data):
    """å¤šæ ·åŒ–æ•°å€¼è¡¨è¾¾"""
    import random

    if preserve_data:
        # åªæ”¹å˜è¡¨è¾¾å½¢å¼,ä¸æ”¹å˜æ•°å€¼
        value = float(expression["value"])
        unit = expression["unit"]

        alternatives = []

        if expression["type"] == "area":
            # é¢ç§¯è¡¨è¾¾
            if value >= 10000:
                alternatives = [
                    f"{value}å¹³æ–¹ç±³",
                    f"{value}ã¡",
                    f"{int(value/10000)}ä¸‡å¹³æ–¹ç±³",
                    f"{int(value/10000)}ä¸‡mÂ²"
                ]
            else:
                alternatives = [
                    f"{int(value)}å¹³æ–¹ç±³",
                    f"{int(value)}ã¡",
                    f"{int(value)}mÂ²"
                ]

        elif expression["type"] == "money":
            # é‡‘é¢è¡¨è¾¾
            if "ä¸‡å…ƒ" in unit:
                alternatives = [
                    f"{value}ä¸‡å…ƒ",
                    f"{int(value)}ä¸‡å…ƒ",
                    f"{value}ä¸‡"
                ]
            elif "äº¿å…ƒ" in unit:
                alternatives = [
                    f"{value}äº¿å…ƒ",
                    f"{value}äº¿"
                ]

        elif expression["type"] == "time":
            # æ—¶é—´è¡¨è¾¾
            if "ä¸ªæœˆ" in unit or "æœˆ" in unit:
                months = int(value)
                alternatives = [
                    f"{months}ä¸ªæœˆ",
                    f"{months}æœˆ"
                ]
                if months % 12 == 0:
                    years = months // 12
                    alternatives.append(f"{years}å¹´")

        # éšæœºé€‰æ‹©ä¸€ä¸ªæ›¿ä»£è¡¨è¾¾
        return random.choice(alternatives) if alternatives else expression["original"]
    else:
        # ä¸ä¿æŠ¤æ•°æ®,å¯ä»¥è¿›è¡Œæ›´çµæ´»çš„æ”¹å†™
        return expression["original"]
```

**ç¤ºä¾‹æ•ˆæœ**:
```
åŸæ–‡: "æ€»å»ºç­‘é¢ç§¯50000å¹³æ–¹ç±³"

å¤šæ ·åŒ–è¡¨è¾¾:
- "æ€»å»ºç­‘é¢ç§¯50000ã¡"
- "æ€»å»ºç­‘é¢ç§¯5ä¸‡å¹³æ–¹ç±³"
- "å»ºç­‘é¢ç§¯çº¦5ä¸‡mÂ²"
- "å»ºç­‘æ€»é¢ç§¯è¾¾50000å¹³æ–¹ç±³"

åŸæ–‡: "å»ºè®¾å·¥æœŸ24ä¸ªæœˆ"

å¤šæ ·åŒ–è¡¨è¾¾:
- "å»ºè®¾å·¥æœŸ24æœˆ"
- "å»ºè®¾å·¥æœŸä¸¤å¹´"
- "è®¡åˆ’å·¥æœŸ24ä¸ªæœˆ"
- "å·¥æœŸä¸º2å¹´"
```

---

### é˜¶æ®µ 6: ç­–ç•¥ 5 - æ•´ä½“ä¼˜åŒ–ä¸è´¨é‡æ£€æŸ¥

#### æ­¥éª¤ 6.1: ä¸“ä¸šæœ¯è¯­ä¸€è‡´æ€§æ£€æŸ¥

```python
def check_term_consistency(text, preserved_terms):
    """æ£€æŸ¥ä¸“ä¸šæœ¯è¯­ä¸€è‡´æ€§"""

    issues = []

    # æ£€æŸ¥æ˜¯å¦æœ‰æœ¯è¯­è¢«è¯¯æ”¹
    for term_info in preserved_terms:
        term = term_info["term"]
        count_original = term_info["count"]
        count_current = text.count(term)

        if count_current != count_original:
            issues.append({
                "term": term,
                "expected": count_original,
                "actual": count_current,
                "severity": "high"
            })

    return issues
```

#### æ­¥éª¤ 6.2: æ•°å€¼å‡†ç¡®æ€§æ£€æŸ¥

```python
def check_data_accuracy(original_text, diversified_text, preserve_data):
    """æ£€æŸ¥æ•°å€¼å‡†ç¡®æ€§"""

    if not preserve_data:
        return []  # ä¸è¦æ±‚ä¿ç•™æ•°æ®,è·³è¿‡æ£€æŸ¥

    import re

    # æå–åŸæ–‡ä¸­çš„æ‰€æœ‰æ•°å€¼
    original_numbers = re.findall(r'\d+(?:\.\d+)?', original_text)
    diversified_numbers = re.findall(r'\d+(?:\.\d+)?', diversified_text)

    issues = []

    # æ£€æŸ¥æ•°å€¼æ•°é‡æ˜¯å¦ä¸€è‡´
    if len(original_numbers) != len(diversified_numbers):
        issues.append({
            "issue": "æ•°å€¼æ•°é‡ä¸ä¸€è‡´",
            "expected": len(original_numbers),
            "actual": len(diversified_numbers),
            "severity": "high"
        })

    # æ£€æŸ¥å…³é”®æ•°å€¼æ˜¯å¦ä¿ç•™
    for num in original_numbers:
        if num not in diversified_numbers:
            issues.append({
                "issue": f"æ•°å€¼ {num} ä¸¢å¤±æˆ–æ”¹å˜",
                "severity": "high"
            })

    return issues
```

#### æ­¥éª¤ 6.3: å¯è¯»æ€§è¯„ä¼°

```python
def evaluate_readability(text):
    """è¯„ä¼°å¯è¯»æ€§"""

    metrics = {
        "avg_sentence_length": 0,
        "avg_paragraph_length": 0,
        "readability_score": 0
    }

    # å¥å­é•¿åº¦
    sentences = text.split('ã€‚')
    sentences = [s.strip() for s in sentences if s.strip()]
    if sentences:
        metrics["avg_sentence_length"] = sum(len(s) for s in sentences) / len(sentences)

    # æ®µè½é•¿åº¦
    paragraphs = text.split('\n\n')
    paragraphs = [p.strip() for p in paragraphs if p.strip()]
    if paragraphs:
        metrics["avg_paragraph_length"] = sum(len(p) for p in paragraphs) / len(paragraphs)

    # ç®€åŒ–çš„å¯è¯»æ€§è¯„åˆ†(åŸºäºå¥å­é•¿åº¦)
    if metrics["avg_sentence_length"] < 20:
        metrics["readability_score"] = 90  # æ˜“è¯»
    elif metrics["avg_sentence_length"] < 30:
        metrics["readability_score"] = 75  # ä¸­ç­‰
    else:
        metrics["readability_score"] = 60  # è¾ƒéš¾

    return metrics
```

#### æ­¥éª¤ 6.4: è®¡ç®—ç›¸ä¼¼åº¦

```python
def calculate_similarity(original_text, diversified_text):
    """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦(ç®€åŒ–ç®—æ³•)"""

    # ç®€åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—:åŸºäºå­—ç¬¦çº§åˆ«çš„Jaccardç›¸ä¼¼åº¦
    def get_char_ngrams(text, n=3):
        """æå–å­—ç¬¦n-gram"""
        return set(text[i:i+n] for i in range(len(text) - n + 1))

    original_ngrams = get_char_ngrams(original_text)
    diversified_ngrams = get_char_ngrams(diversified_text)

    if not original_ngrams or not diversified_ngrams:
        return 1.0

    intersection = original_ngrams & diversified_ngrams
    union = original_ngrams | diversified_ngrams

    similarity = len(intersection) / len(union) if union else 1.0

    return similarity
```

---

## å®Œæ•´æ‰§è¡Œæµç¨‹

### ä¸»å‡½æ•°

```python
def diversify_content(input_text, diversification_level="medium",
                     preserve_data=True, preserve_terms=None,
                     target_similarity=0.2, preserve_structure=True):
    """
    ä¸»å‡½æ•°:æ‰§è¡Œå†…å®¹å¤šæ ·åŒ–æ”¹å†™

    Args:
        input_text: è¾“å…¥æ–‡æœ¬
        diversification_level: æ”¹å†™å¼ºåº¦ (low/medium/high)
        preserve_data: æ˜¯å¦ä¿ç•™æ•°å€¼æ•°æ®
        preserve_terms: éœ€è¦ä¿ç•™çš„ä¸“ä¸šæœ¯è¯­åˆ—è¡¨
        target_similarity: ç›®æ ‡ç›¸ä¼¼åº¦
        preserve_structure: æ˜¯å¦ä¿ç•™æ®µè½ç»“æ„

    Returns:
        dict: æ”¹å†™ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
    """

    if preserve_terms is None:
        preserve_terms = PROTECTED_TERMS

    result = {
        "diversified_text": "",
        "statistics": {},
        "quality_checks": {},
        "warnings": []
    }

    # é˜¶æ®µ1: æ–‡æœ¬é¢„å¤„ç†
    structure = analyze_text_structure(input_text)
    protected_elements = extract_protected_elements(
        input_text, preserve_terms, preserve_data
    )

    # é˜¶æ®µ2: åŒä¹‰è¯æ›¿æ¢
    text_after_synonym, synonym_replacements = apply_synonym_replacement(
        input_text, protected_elements, diversification_level
    )

    # é˜¶æ®µ3: å¥å¼é‡ç»„
    sentences = text_after_synonym.split('ã€‚')
    restructured_sentences = []
    sentence_restructures = 0

    for sent in sentences:
        if sent.strip():
            struct_type = identify_sentence_structure(sent)
            restructured = restructure_sentence(sent, struct_type)

            if restructured != sent:
                sentence_restructures += 1

            restructured_sentences.append(restructured)

    text_after_restructure = 'ã€‚'.join(restructured_sentences)

    # é˜¶æ®µ4: æ®µè½é‡æ„
    if structure["structure_type"] == "multi_paragraph":
        paragraphs = text_after_restructure.split('\n\n')
        reconstructed_paragraphs = []
        paragraph_reconstructs = 0

        for para in paragraphs:
            if para.strip():
                reconstructed = reconstruct_paragraph(para, preserve_structure)
                if reconstructed != para:
                    paragraph_reconstructs += 1
                reconstructed_paragraphs.append(reconstructed)

        text_after_paragraph = '\n\n'.join(reconstructed_paragraphs)
    else:
        text_after_paragraph = text_after_restructure
        paragraph_reconstructs = 0

    # é˜¶æ®µ5: æ•°å€¼è¡¨è¾¾å¤šæ ·åŒ–
    numerical_exprs = identify_numerical_expressions(text_after_paragraph)
    final_text = text_after_paragraph

    for expr in reversed(numerical_exprs):  # ä»åå‘å‰æ›¿æ¢,é¿å…ä½ç½®åç§»
        diversified_expr = diversify_numerical_expression(expr, preserve_data)
        final_text = (
            final_text[:expr["start"]] +
            diversified_expr +
            final_text[expr["end"]:]
        )

    # é˜¶æ®µ6: è´¨é‡æ£€æŸ¥
    term_issues = check_term_consistency(final_text, protected_elements["terms"])
    data_issues = check_data_accuracy(input_text, final_text, preserve_data)
    readability = evaluate_readability(final_text)
    similarity = calculate_similarity(input_text, final_text)

    # ç»„è£…ç»“æœ
    result["diversified_text"] = final_text
    result["statistics"] = {
        "synonym_replacements": len(synonym_replacements),
        "sentence_restructures": sentence_restructures,
        "paragraph_reconstructs": paragraph_reconstructs,
        "numerical_diversifications": len(numerical_exprs),
        "similarity_score": similarity,
        "target_similarity": target_similarity,
        "similarity_reduction": 1 - similarity
    }
    result["quality_checks"] = {
        "term_consistency_issues": term_issues,
        "data_accuracy_issues": data_issues,
        "readability_metrics": readability,
        "overall_pass": len(term_issues) == 0 and len(data_issues) == 0
    }

    # ç”Ÿæˆè­¦å‘Š
    if similarity > target_similarity:
        result["warnings"].append(
            f"ç›¸ä¼¼åº¦ {similarity:.2%} é«˜äºç›®æ ‡å€¼ {target_similarity:.2%},å»ºè®®æé«˜æ”¹å†™å¼ºåº¦"
        )

    if term_issues:
        result["warnings"].append(
            f"å‘ç° {len(term_issues)} ä¸ªæœ¯è¯­ä¸€è‡´æ€§é—®é¢˜"
        )

    if data_issues:
        result["warnings"].append(
            f"å‘ç° {len(data_issues)} ä¸ªæ•°æ®å‡†ç¡®æ€§é—®é¢˜"
        )

    return result
```

---

## ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1: åŸºç¡€ä½¿ç”¨

```python
input_text = """
æœ¬é¡¹ç›®é‡‡ç”¨BIMæŠ€æœ¯è¿›è¡Œä¸‰ç»´å»ºæ¨¡,é€šè¿‡æ•°å­—åŒ–æ‰‹æ®µæé«˜è®¾è®¡è´¨é‡ã€‚
é¡¹ç›®æ€»å»ºç­‘é¢ç§¯50000å¹³æ–¹ç±³,å»ºè®¾å·¥æœŸ24ä¸ªæœˆã€‚
é‡‡ç”¨è£…é…å¼å»ºç­‘æŠ€æœ¯,å®ç°ç»¿è‰²å»ºé€ ç›®æ ‡ã€‚
"""

result = diversify_content(
    input_text=input_text,
    diversification_level="medium",
    preserve_data=True,
    preserve_terms=["BIMæŠ€æœ¯", "è£…é…å¼å»ºç­‘", "ç»¿è‰²å»ºé€ "],
    target_similarity=0.2
)

print("æ”¹å†™åæ–‡æœ¬:")
print(result["diversified_text"])
print("\nç»Ÿè®¡ä¿¡æ¯:")
print(f"åŒä¹‰æ›¿æ¢: {result['statistics']['synonym_replacements']} å¤„")
print(f"å¥å¼é‡ç»„: {result['statistics']['sentence_restructures']} å¤„")
print(f"ç›¸ä¼¼åº¦: {result['statistics']['similarity_score']:.2%}")
```

**è¾“å‡º**:
```
æ”¹å†™åæ–‡æœ¬:
æœ¬å·¥ç¨‹è¿ç”¨BIMæŠ€æœ¯å¼€å±•ç«‹ä½“åŒ–å»ºæ¨¡,å€ŸåŠ©æ•°å­—åŒ–æ‰‹æ®µæå‡è®¾è®¡å“è´¨ã€‚
é¡¹ç›®å»ºç­‘æ€»é¢ç§¯5ä¸‡å¹³æ–¹ç±³,è®¡åˆ’å·¥æœŸ24æœˆã€‚
åº”ç”¨è£…é…å¼å»ºç­‘æŠ€æœ¯,è¾¾æˆç»¿è‰²å»ºé€ ç›®æ ‡ã€‚

ç»Ÿè®¡ä¿¡æ¯:
åŒä¹‰æ›¿æ¢: 8 å¤„
å¥å¼é‡ç»„: 2 å¤„
ç›¸ä¼¼åº¦: 18.5%
```

### ç¤ºä¾‹ 2: é«˜å¼ºåº¦æ”¹å†™

```python
result = diversify_content(
    input_text=input_text,
    diversification_level="high",  # é«˜å¼ºåº¦
    preserve_data=True,
    preserve_terms=["BIMæŠ€æœ¯", "è£…é…å¼å»ºç­‘"],
    target_similarity=0.15  # æ›´ä½çš„ç›¸ä¼¼åº¦ç›®æ ‡
)
```

### ç¤ºä¾‹ 3: æ‰¹é‡æ”¹å†™(ç”Ÿæˆå¤šä¸ªç‰ˆæœ¬)

```python
def generate_multiple_versions(input_text, num_versions=3):
    """ç”Ÿæˆå¤šä¸ªä¸åŒç‰ˆæœ¬"""

    versions = []
    for i in range(num_versions):
        result = diversify_content(
            input_text=input_text,
            diversification_level="high",
            preserve_data=True,
            preserve_terms=PROTECTED_TERMS,
            target_similarity=0.2
        )
        versions.append({
            "version": i + 1,
            "text": result["diversified_text"],
            "similarity": result["statistics"]["similarity_score"]
        })

    return versions
```

---

## è¾“å‡ºæ ¼å¼

```json
{
  "diversified_text": "æ”¹å†™åçš„å®Œæ•´æ–‡æœ¬...",
  "statistics": {
    "synonym_replacements": 45,
    "sentence_restructures": 12,
    "paragraph_reconstructs": 5,
    "numerical_diversifications": 8,
    "similarity_score": 0.18,
    "target_similarity": 0.20,
    "similarity_reduction": 0.82
  },
  "quality_checks": {
    "term_consistency_issues": [],
    "data_accuracy_issues": [],
    "readability_metrics": {
      "avg_sentence_length": 22.5,
      "avg_paragraph_length": 156.3,
      "readability_score": 75
    },
    "overall_pass": true
  },
  "warnings": []
}
```

---

## æ³¨æ„äº‹é¡¹

### âš ï¸  é‡è¦æé†’

1. **ä¸“ä¸šæœ¯è¯­ä¿æŠ¤**
   - BIMã€æ™ºæ…§å·¥åœ°ç­‰ä¸“ä¸šæœ¯è¯­ä¸åº”æ”¹å†™
   - è¡Œä¸šæ ‡å‡†ç¼–å·(GBã€JGJ)å¿…é¡»ä¿æŒåŸæ ·
   - æŠ€æœ¯å‚æ•°æœ¯è¯­éœ€ä¿æŒå‡†ç¡®æ€§

2. **æ•°å€¼æ•°æ®å‡†ç¡®æ€§**
   - é¢ç§¯ã€é‡‘é¢ã€å·¥æœŸç­‰æ•°å€¼ä¸å¾—æ”¹å˜
   - åªæ”¹å˜è¡¨è¾¾å½¢å¼,ä¸æ”¹å˜æ•°å€¼æœ¬èº«
   - é‡è¦æ•°æ®å»ºè®®äººå·¥å¤æ ¸

3. **é€»è¾‘è¿è´¯æ€§**
   - æ”¹å†™åçš„æ–‡æœ¬åº”ä¿æŒé€»è¾‘æ¸…æ™°
   - å› æœå…³ç³»ã€è½¬æŠ˜å…³ç³»åº”å‡†ç¡®è¡¨è¾¾
   - é¿å…è¯­ä¹‰æ¨¡ç³Šæˆ–æ­§ä¹‰

4. **æŸ¥é‡ç‡è¯´æ˜**
   - ç›¸ä¼¼åº¦ 15-20% å¯¹åº”æŸ¥é‡ç‡çº¦ 80-85%
   - å®é™…æŸ¥é‡ç‡å—æ£€æµ‹å·¥å…·å½±å“
   - å»ºè®®ä½¿ç”¨ä¸“ä¸šæŸ¥é‡å·¥å…·éªŒè¯

5. **äººå·¥å®¡é˜…**
   - æ”¹å†™åçš„å†…å®¹åº”ç”±ä¸“ä¸šäººå‘˜å®¡é˜…
   - é‡ç‚¹æ£€æŸ¥æŠ€æœ¯å‡†ç¡®æ€§å’Œå¯è¯»æ€§
   - å¿…è¦æ—¶è¿›è¡Œäººå·¥è°ƒæ•´

### ğŸ”§ æ€§èƒ½ä¼˜åŒ–

- **å°æ–‡æœ¬**(< 1000å­—): < 1ç§’
- **ä¸­æ–‡æœ¬**(1000-5000å­—): 1-3ç§’
- **å¤§æ–‡æœ¬**(> 5000å­—): 3-10ç§’

### ğŸ“š ç›¸å…³æŠ€èƒ½

- `smart-retrieval` - æ™ºèƒ½æ–‡æ¡£æ£€ç´¢
- `context-builder` - ä¸Šä¸‹æ–‡æ„å»º
- `citation-manager` - å¼•ç”¨ç®¡ç†
- `format-analyzer` - æ ¼å¼åˆ†æ

---

**ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-11-06
**ä½œè€…**: NotebookLM Assistant Team
