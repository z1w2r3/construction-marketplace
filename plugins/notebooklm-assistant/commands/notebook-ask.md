# NotebookLM æ™ºèƒ½é—®ç­”

åŸºäºçŸ¥è¯†åº“æ™ºèƒ½å›ç­”ç”¨æˆ·é—®é¢˜(åˆ©ç”¨é¢„è§ˆç´¢å¼•åŠ é€Ÿæ£€ç´¢)ã€‚

---

**ç”¨æˆ·é—®é¢˜**: $ARGUMENTS

---

## æ‰§è¡Œæµç¨‹

### 1. ç†è§£é—®é¢˜æ„å›¾

åˆ†æé—®é¢˜ç±»å‹:
- **äº‹å®æŸ¥è¯¢**: "XX æ˜¯ä»€ä¹ˆ?"ã€"é¡¹ç›®é¢„ç®—æ˜¯å¤šå°‘?"
- **å¯¹æ¯”åˆ†æ**: "A å’Œ B çš„åŒºåˆ«?"ã€"å“ªä¸ªæ–¹æ¡ˆæ›´å¥½?"
- **æ—¶é—´çº¿è¿½æº¯**: "XX å¦‚ä½•å‘å±•çš„?"ã€"è¿›åº¦å˜åŒ–?"
- **ç»¼åˆæ´å¯Ÿ**: "æœ‰å“ªäº›å…³é”®å‘ç°?"ã€"ä¸»è¦é£é™©?"

### 2. ç¬¬ä¸€è½®ç­›é€‰(åŸºäºç´¢å¼•é¢„è§ˆ)

ä» `.notebooklm/index/metadata.json` è¯»å–ç´¢å¼•,åˆ©ç”¨é¢„è§ˆæ•°æ®å¿«é€Ÿç­›é€‰ç›¸å…³æ–‡æ¡£:

```python
import json
import re
from pathlib import Path

# è¯»å–ç´¢å¼•
with open('.notebooklm/index/metadata.json', 'r', encoding='utf-8') as f:
    index_data = json.load(f)

files = index_data['index']
query = "$ARGUMENTS"  # ç”¨æˆ·é—®é¢˜

# æå–é—®é¢˜å…³é”®è¯
def extract_keywords(text):
    """ç®€å•çš„å…³é”®è¯æå–"""
    # ç§»é™¤æ ‡ç‚¹å’Œåœç”¨è¯
    stopwords = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'åŠ', 'ç­‰', 'äº†', 'å—', 'å‘¢', 'ä»€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ'}
    words = re.findall(r'[\u4e00-\u9fa5a-zA-Z0-9]+', text)
    return [w for w in words if w not in stopwords and len(w) > 1]

query_keywords = extract_keywords(query)
print(f"ğŸ” é—®é¢˜å…³é”®è¯: {', '.join(query_keywords)}")

# è®¡ç®—ç›¸å…³åº¦å¾—åˆ†
def calculate_relevance(file, keywords):
    """åŸºäºé¢„è§ˆæ•°æ®è®¡ç®—ç›¸å…³åº¦"""
    score = 0
    reasons = []

    # æ–‡ä»¶ååŒ¹é…(æƒé‡ 2)
    for kw in keywords:
        if kw.lower() in file['name'].lower():
            score += 2
            reasons.append(f"æ–‡ä»¶ååŒ…å«'{kw}'")

    # é¢„è§ˆå†…å®¹åŒ¹é…(æƒé‡ 3)
    preview_text = file.get('preview', '')
    for kw in keywords:
        if kw in preview_text:
            count = preview_text.count(kw)
            score += min(count * 3, 9)  # å•ä¸ªå…³é”®è¯æœ€å¤šåŠ 9åˆ†
            reasons.append(f"å†…å®¹åŒ…å«'{kw}'({count}æ¬¡)")

    # PDF ç›®å½•åŒ¹é…(æƒé‡ 2)
    if file.get('has_toc'):
        toc_text = ' '.join(file.get('toc', []))
        for kw in keywords:
            if kw in toc_text:
                score += 2
                reasons.append(f"ç›®å½•åŒ…å«'{kw}'")

    # Word æ ‡é¢˜åŒ¹é…(æƒé‡ 2)
    if file.get('headings'):
        headings_text = ' '.join([h['text'] for h in file['headings']])
        for kw in keywords:
            if kw in headings_text:
                score += 2
                reasons.append(f"æ ‡é¢˜åŒ…å«'{kw}'")

    # Excel Sheet åç§°åŒ¹é…(æƒé‡ 1)
    if file.get('sheet_names'):
        sheets_text = ' '.join(file['sheet_names'])
        for kw in keywords:
            if kw in sheets_text:
                score += 1
                reasons.append(f"SheetååŒ…å«'{kw}'")

    # æœ€æ–°æ–‡æ¡£åŠ åˆ†(æƒé‡ 1)
    import time
    days_old = (time.time() - file['modified']) / (24 * 3600)
    if days_old < 30:
        score += 1
        reasons.append("æœ€è¿‘30å¤©å†…çš„æ–‡æ¡£")

    return score, reasons

# å¯¹æ‰€æœ‰æ–‡æ¡£è®¡ç®—ç›¸å…³åº¦
scored_files = []
for file in files:
    score, reasons = calculate_relevance(file, query_keywords)
    if score > 0:  # åªä¿ç•™æœ‰ç›¸å…³æ€§çš„
        file_with_score = file.copy()
        file_with_score['relevance_score'] = score
        file_with_score['relevance_reasons'] = reasons
        scored_files.append(file_with_score)

# æŒ‰ç›¸å…³åº¦æ’åº
scored_files_sorted = sorted(scored_files, key=lambda f: f['relevance_score'], reverse=True)

# ç­›é€‰ top 10 å€™é€‰æ–‡æ¡£
top_candidates = scored_files_sorted[:10]

print(f"\nğŸ“Š åˆç­›ç»“æœ: ä» {len(files)} ä¸ªæ–‡æ¡£ä¸­ç­›é€‰å‡º {len(top_candidates)} ä¸ªå€™é€‰æ–‡æ¡£")
for i, f in enumerate(top_candidates[:5], 1):
    print(f"   {i}. {f['name']} (å¾—åˆ†: {f['relevance_score']}, åŸå› : {', '.join(f['relevance_reasons'][:2])})")
```

### 3. ç¬¬äºŒè½®ç²¾å‡†åŒ¹é…(æ·±åº¦è§£æ)

å¯¹ top 10 å€™é€‰æ–‡æ¡£è¿›è¡Œå®Œæ•´è§£æ,ä½¿ç”¨å¯¹åº”çš„ **pdf/docx/xlsx Skills**:

```python
import pdfplumber
from docx import Document
from openpyxl import load_workbook

def parse_pdf_full(pdf_path):
    """å®Œæ•´è§£æ PDF"""
    with pdfplumber.open(pdf_path) as pdf:
        full_text = ""
        for page in pdf.pages:
            full_text += page.extract_text() or ""
        return full_text

def parse_docx_full(docx_path):
    """å®Œæ•´è§£æ Word"""
    doc = Document(docx_path)
    full_text = "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
    return full_text

def parse_xlsx_full(xlsx_path):
    """å®Œæ•´è§£æ Excel(ä¸»è¦ sheet)"""
    wb = load_workbook(xlsx_path, read_only=True, data_only=True)
    full_text = ""

    for sheet_name in wb.sheetnames[:3]:  # åªè§£æå‰3ä¸ªsheet
        ws = wb[sheet_name]
        full_text += f"\n=== Sheet: {sheet_name} ===\n"

        for row in ws.iter_rows(values_only=True, max_row=100):  # æ¯ä¸ªsheetæœ€å¤š100è¡Œ
            row_text = '\t'.join([str(cell) if cell else '' for cell in row])
            full_text += row_text + "\n"

    wb.close()
    return full_text

# å¯¹å€™é€‰æ–‡æ¡£è¿›è¡Œå®Œæ•´è§£æ
parsed_docs = []
for file in top_candidates:
    try:
        if file['extension'] == '.pdf':
            content = parse_pdf_full(file['path'])
        elif file['extension'] in ['.docx', '.doc']:
            content = parse_docx_full(file['path'])
        elif file['extension'] in ['.xlsx', '.xls']:
            content = parse_xlsx_full(file['path'])
        else:
            content = file.get('preview', '')

        parsed_docs.append({
            "file": file,
            "content": content[:50000]  # é™åˆ¶é•¿åº¦,é¿å…è¶…token
        })

        print(f"âœ… å·²è§£æ: {file['name']} ({len(content)} å­—ç¬¦)")

    except Exception as e:
        print(f"âš ï¸  è§£æå¤±è´¥: {file['name']} - {e}")
        continue
```

### 4. ä½¿ç”¨ Context Builder Skill æ„å»ºä¸Šä¸‹æ–‡

ä½¿ç”¨ Skill tool è°ƒç”¨ context-builder:

```python
# è°ƒç”¨ context-builder skill æ„å»ºä¸Šä¸‹æ–‡
# æ³¨æ„:è¿™é‡Œåº”è¯¥ä½¿ç”¨ Skill tool,è€Œä¸æ˜¯ç›´æ¥è°ƒç”¨ä»£ç 

# ä¼ªä»£ç ç¤ºæ„:
# context = Skill("notebooklm-assistant:context-builder", {
#     "documents": parsed_docs,
#     "query": query,
#     "max_tokens": 10000
# })

# æ‰‹åŠ¨å®ç°ç®€åŒ–ç‰ˆä¸Šä¸‹æ–‡æ„å»º:
def build_context(docs, query, max_tokens=10000):
    """ç®€åŒ–çš„ä¸Šä¸‹æ–‡æ„å»º"""
    context_parts = []
    total_tokens = 0

    for doc in docs:
        # æå–åŒ…å«å…³é”®è¯çš„æ®µè½
        content = doc['content']
        relevant_paragraphs = []

        for paragraph in content.split('\n'):
            if any(kw in paragraph for kw in query_keywords):
                relevant_paragraphs.append(paragraph)

        # æ„å»ºæ–‡æ¡£ä¸Šä¸‹æ–‡
        if relevant_paragraphs:
            doc_context = f"\n## æ–‡æ¡£: {doc['file']['name']}\n"
            doc_context += '\n'.join(relevant_paragraphs[:5])  # æœ€å¤š5æ®µ

            # ä¼°ç®—token(ç²—ç•¥:1å­—ç¬¦â‰ˆ0.5token)
            estimated_tokens = len(doc_context) // 2
            if total_tokens + estimated_tokens < max_tokens:
                context_parts.append(doc_context)
                total_tokens += estimated_tokens

    return '\n'.join(context_parts)

context = build_context(parsed_docs, query)
print(f"\nğŸ“ å·²æ„å»ºä¸Šä¸‹æ–‡: {len(context)} å­—ç¬¦ (çº¦ {len(context)//2} tokens)")
```

### 5. ç”Ÿæˆç­”æ¡ˆ

åŸºäºæ„å»ºçš„ä¸Šä¸‹æ–‡,Claude ç»¼åˆåˆ†æå¹¶å›ç­”é—®é¢˜:

**é‡è¦æç¤º**:
- ç›´æ¥å¼•ç”¨åŸæ–‡(å¸¦æ–‡ä»¶åå’Œä½ç½®)
- ç»¼åˆå¤šä¸ªæ¥æº
- çªå‡ºé‡ç‚¹
- å®¢è§‚é™ˆè¿°
- **æ‰€æœ‰ä¿¡æ¯å¿…é¡»åŸºäºä¸Šä¸‹æ–‡ä¸­çš„æ–‡æ¡£,ä¸ç¼–é€ **

å‚è€ƒä¸Šä¸‹æ–‡,ç”Ÿæˆç»“æ„åŒ–ç­”æ¡ˆ:

```markdown
## ğŸ¤” æ‚¨çš„é—®é¢˜
[å¤è¿°ç”¨æˆ·é—®é¢˜]

## âœ… ç­”æ¡ˆ
[åŸºäºæ–‡æ¡£çš„å›ç­”,æ®µè½å½¢å¼]

æ ¹æ®æ–‡æ¡£åˆ†æ,[æ ¸å¿ƒç­”æ¡ˆ]...

å…·ä½“æ¥è¯´:
1. [è¦ç‚¹1]
   > "[åŸæ–‡å¼•ç”¨]"
   > ğŸ“„ æ¥æº: æ–‡æ¡£å.pdf

2. [è¦ç‚¹2]
   > "[åŸæ–‡å¼•ç”¨]"
   > ğŸ“„ æ¥æº: æ–‡æ¡£å.docx

### ğŸ“Š å…³é”®æ•°æ®(å¦‚æœæœ‰)
| æŒ‡æ ‡ | æ•°å€¼ | æ¥æº |
|------|------|------|
| XX | YY | æ–‡æ¡£A |

### ğŸ“„ ä¿¡æ¯æ¥æº
1. [æ–‡æ¡£å1](è·¯å¾„) - ç›¸å…³åº¦: 95%
2. [æ–‡æ¡£å2](è·¯å¾„) - ç›¸å…³åº¦: 87%
3. ...

## ğŸ’¡ æ‚¨å¯èƒ½è¿˜æƒ³äº†è§£
- [ç›¸å…³é—®é¢˜1]
- [ç›¸å…³é—®é¢˜2]
- [ç›¸å…³é—®é¢˜3]

## ğŸ” æ·±å…¥æ¢ç´¢
- æ·±åº¦ç ”ç©¶: `/notebook-research [ä¸»é¢˜]`
- ç”ŸæˆæŠ¥å‘Š: `/notebook-report analysis "[ä¸»é¢˜]"`
```

### 6. æ™ºèƒ½ç¼“å­˜(å¯é€‰ä¼˜åŒ–)

å°†è§£æç»“æœç¼“å­˜åˆ° `.notebooklm/cache/` é¿å…é‡å¤è§£æ:

```python
import hashlib
from pathlib import Path

# ç¼“å­˜ç›®å½•
cache_dir = Path('.notebooklm/cache')
cache_dir.mkdir(parents=True, exist_ok=True)

# ç”Ÿæˆæ–‡ä»¶å“ˆå¸Œä½œä¸ºç¼“å­˜key
def get_file_hash(file_path, modified_time):
    """åŸºäºè·¯å¾„å’Œä¿®æ”¹æ—¶é—´ç”Ÿæˆå“ˆå¸Œ"""
    key = f"{file_path}_{modified_time}"
    return hashlib.md5(key.encode()).hexdigest()

# ä¿å­˜ç¼“å­˜
for doc in parsed_docs:
    file = doc['file']
    cache_key = get_file_hash(file['path'], file['modified'])
    cache_file = cache_dir / f"{cache_key}.json"

    with open(cache_file, 'w', encoding='utf-8') as f:
        json.dump({
            "file_path": file['path'],
            "file_name": file['name'],
            "modified": file['modified'],
            "content": doc['content']
        }, f, ensure_ascii=False)

print(f"ğŸ’¾ å·²ç¼“å­˜ {len(parsed_docs)} ä¸ªæ–‡æ¡£åˆ° .notebooklm/cache/")
```

---

## æ™ºèƒ½æ£€ç´¢åŸç†(ä¼˜åŒ–å)

### ç¬¬ä¸€è½®: åŸºäºç´¢å¼•é¢„è§ˆçš„å¿«é€Ÿç­›é€‰
- ä» metadata.json è¯»å–æ‰€æœ‰æ–‡æ¡£çš„é¢„è§ˆæ•°æ®
- åŒ¹é…æ–‡ä»¶åã€é¢„è§ˆå†…å®¹ã€ç›®å½•ã€æ ‡é¢˜ã€Sheetåç§°
- è®¡ç®—ç»¼åˆç›¸å…³åº¦å¾—åˆ†
- **ä¼˜åŠ¿**: æ— éœ€æ‰“å¼€æ–‡ä»¶,æå¿«é€Ÿ(æ¯«ç§’çº§)

### ç¬¬äºŒè½®: æ·±åº¦è§£æ top å€™é€‰æ–‡æ¡£
- åªå¯¹ç›¸å…³åº¦æœ€é«˜çš„ 10 ä¸ªæ–‡æ¡£è¿›è¡Œå®Œæ•´è§£æ
- ä½¿ç”¨å¯¹åº”çš„ pdf/docx/xlsx Skills
- æå–åŒ…å«å…³é”®è¯çš„æ®µè½
- **ä¼˜åŠ¿**: Token æ¶ˆè€—å¯æ§,å‡†ç¡®åº¦é«˜

### ç¬¬ä¸‰è½®: ä¸Šä¸‹æ–‡æ„å»ºå’Œç­”æ¡ˆç”Ÿæˆ
- ä½¿ç”¨ context-builder skill æ•´åˆä¿¡æ¯
- Claude åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ
- å¼•ç”¨æ¥æº,ä¿è¯å¯è¿½æº¯

---

## æ€§èƒ½ä¼˜åŒ–

- **ä¸¤è½®æ£€ç´¢**: å…ˆç´¢å¼•ç­›é€‰(å¿«),å†æ·±åº¦è§£æ(å‡†)
- **Token èŠ‚çœ**: åªè¯»å–ç›¸å…³æ–‡æ¡£,é¿å…å…¨é‡åŠ è½½
- **æ™ºèƒ½ç¼“å­˜**: å·²è§£ææ–‡æ¡£ç¼“å­˜å¤ç”¨
- **è¶…æ—¶ä¿æŠ¤**: å•ä¸ªæ–‡æ¡£è§£æè¶…æ—¶è·³è¿‡

---

## ç¤ºä¾‹

**ç”¨æˆ·**: `/notebook-ask é¡¹ç›®çš„ä¸»è¦é£é™©æ˜¯ä»€ä¹ˆ?`

**è¾“å‡º**:
```
ğŸ” é—®é¢˜å…³é”®è¯: é¡¹ç›®, ä¸»è¦, é£é™©

ğŸ“Š åˆç­›ç»“æœ: ä» 100 ä¸ªæ–‡æ¡£ä¸­ç­›é€‰å‡º 8 ä¸ªå€™é€‰æ–‡æ¡£
   1. risk-assessment.pdf (å¾—åˆ†: 15, åŸå› : æ–‡ä»¶ååŒ…å«'é£é™©', å†…å®¹åŒ…å«'é£é™©'(5æ¬¡))
   2. project-plan.docx (å¾—åˆ†: 8, åŸå› : å†…å®¹åŒ…å«'é¡¹ç›®'(3æ¬¡), å†…å®¹åŒ…å«'é£é™©'(2æ¬¡))
   3. meeting-notes-2024.txt (å¾—åˆ†: 5, åŸå› : å†…å®¹åŒ…å«'é£é™©'(1æ¬¡), æœ€è¿‘30å¤©å†…çš„æ–‡æ¡£)
   ...

âœ… å·²è§£æ: risk-assessment.pdf (15234 å­—ç¬¦)
âœ… å·²è§£æ: project-plan.docx (8567 å­—ç¬¦)
âœ… å·²è§£æ: meeting-notes-2024.txt (3421 å­—ç¬¦)

ğŸ“ å·²æ„å»ºä¸Šä¸‹æ–‡: 5680 å­—ç¬¦ (çº¦ 2840 tokens)

---

## ğŸ¤” æ‚¨çš„é—®é¢˜
é¡¹ç›®çš„ä¸»è¦é£é™©æ˜¯ä»€ä¹ˆ?

## âœ… ç­”æ¡ˆ
æ ¹æ®æ–‡æ¡£åˆ†æ,é¡¹ç›®é¢ä¸´ä¸‰å¤§ä¸»è¦é£é™©:

1. **é¢„ç®—è¶…æ”¯é£é™©**(é«˜ä¼˜å…ˆçº§)
   > "å½“å‰é¢„ç®—å‚¨å¤‡ä»… 5%,ä½äºè¡Œä¸šæ ‡å‡† 10-15%"
   > ğŸ“„ æ¥æº: risk-assessment.pdf

2. **è¿›åº¦å»¶æœŸé£é™©**(ä¸­ä¼˜å…ˆçº§)
   > "å…³é”®è·¯å¾„ä¸Šçš„æ¨¡å— B ä¾èµ–å¤–éƒ¨ä¾›åº”å•†,äº¤ä»˜å­˜åœ¨ä¸ç¡®å®šæ€§"
   > ğŸ“„ æ¥æº: project-plan.docx

3. **æŠ€æœ¯å€ºåŠ¡é£é™©**(ä¸­ä¼˜å…ˆçº§)
   > "å›¢é˜Ÿåœ¨ä¼šè®®ä¸­æåˆ°ç°æœ‰æ¶æ„éš¾ä»¥æ‰©å±•"
   > ğŸ“„ æ¥æº: meeting-notes-2024.txt

### ğŸ“Š é£é™©è¯„ä¼°æ±‡æ€»
| é£é™©ç±»å‹ | ä¼˜å…ˆçº§ | æåŠé¢‘ç‡ | ç¼“è§£æªæ–½ |
|---------|--------|---------|---------|
| é¢„ç®—è¶…æ”¯ | é«˜ | 3æ¬¡ | å¢åŠ å‚¨å¤‡é‡‘ |
| è¿›åº¦å»¶æœŸ | ä¸­ | 2æ¬¡ | å¯»æ‰¾å¤‡é€‰ä¾›åº”å•† |
| æŠ€æœ¯å€ºåŠ¡ | ä¸­ | 1æ¬¡ | æ¶æ„é‡æ„è®¡åˆ’ |

### ğŸ“„ ä¿¡æ¯æ¥æº
1. [risk-assessment.pdf](path) - ç›¸å…³åº¦: 98%
2. [project-plan.docx](path) - ç›¸å…³åº¦: 85%
3. [meeting-notes-2024.txt](path) - ç›¸å…³åº¦: 72%

## ğŸ’¡ æ‚¨å¯èƒ½è¿˜æƒ³äº†è§£
- è¿™äº›é£é™©çš„å…·ä½“ç¼“è§£æªæ–½æ˜¯ä»€ä¹ˆ?
- å†å²é¡¹ç›®ä¸­ç±»ä¼¼é£é™©æ˜¯å¦‚ä½•è§£å†³çš„?
- æ˜¯å¦æœ‰åº”æ€¥é¢„æ¡ˆæ–‡æ¡£?

## ğŸ” æ·±å…¥æ¢ç´¢
- æ·±åº¦ç ”ç©¶: `/notebook-research é¡¹ç›®é£é™©ç®¡ç†`
- ç”ŸæˆæŠ¥å‘Š: `/notebook-report analysis "é¡¹ç›®é£é™©è¯„ä¼°æŠ¥å‘Š"`
```

---

## æ³¨æ„äº‹é¡¹

- âœ… æ‰€æœ‰ç­”æ¡ˆåŸºäºæ–‡æ¡£,ä¸ç¼–é€ ä¿¡æ¯
- âœ… ä¸ç¡®å®šæ—¶æ˜ç¡®æ ‡æ³¨"éœ€è¦äººå·¥ç¡®è®¤"
- âœ… å¤šä¸ªæ¥æºæœ‰çŸ›ç›¾æ—¶,åˆ—å‡ºæ‰€æœ‰è§‚ç‚¹
- âœ… é‡è¦æ•°æ®å¿…é¡»æ ‡æ³¨æ¥æº
- âœ… åˆ©ç”¨é¢„è§ˆç´¢å¼•å¤§å¹…æå‡æ£€ç´¢é€Ÿåº¦
- âœ… åªæ·±åº¦è§£æé«˜ç›¸å…³æ–‡æ¡£,èŠ‚çœ Token
