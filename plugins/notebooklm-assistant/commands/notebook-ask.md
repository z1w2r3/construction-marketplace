# NotebookLM æ™ºèƒ½é—®ç­”

åŸºäºçŸ¥è¯†åº“æ™ºèƒ½å›ç­”ç”¨æˆ·é—®é¢˜(åˆ©ç”¨é¢„è§ˆç´¢å¼•åŠ é€Ÿæ£€ç´¢)ã€‚

---

**ç”¨æˆ·é—®é¢˜**: $ARGUMENTS

---

## æ ¸å¿ƒåŸåˆ™ â­ æ–°å¢

åœ¨æ•´ä¸ªæ‰§è¡Œè¿‡ç¨‹ä¸­,æˆ‘å°†éµå¾ªä»¥ä¸‹åŸåˆ™:
1. **é€æ˜åŒ–æ€è€ƒ**: æ¯ä¸ªå†³ç­–éƒ½è¯´æ˜ç†ç”±
2. **çœŸå®ä¼˜å…ˆ**: ç»ä¸ç¼–é€ æ•°æ®,æ‰€æœ‰ä¿¡æ¯å¿…é¡»æœ‰æ¥æº
3. **ç”¨æˆ·ç¡®è®¤**: é‡è¦å†³ç­–å‰ç­‰å¾…ç”¨æˆ·ç¡®è®¤
4. **ä¿¡æ¯æº¯æº**: æ‰€æœ‰å¼•ç”¨å¿…é¡»æ ‡æ³¨æ–‡æ¡£æ¥æº

---

## æ‰§è¡Œæµç¨‹

### æ­¥éª¤ 0: ä»»åŠ¡åˆ†ç±»ä¸è·¯ç”± â­ æ–°å¢

#### 0.1 åˆ¤æ–­ä»»åŠ¡ç±»å‹

ğŸ’­ **æˆ‘çš„æ€è€ƒè¿‡ç¨‹**:

åˆ†æç”¨æˆ·é—®é¢˜,åˆ¤æ–­å±äºå“ªç§ç±»å‹:

**Type A: ç®€å•äº‹å®æŸ¥è¯¢(fact-finding)**
- ç¤ºä¾‹: "é¡¹ç›®ç»ç†æ˜¯è°?"ã€"é¢„ç®—æ˜¯å¤šå°‘?"ã€"æˆªæ­¢æ—¥æœŸæ˜¯ä»€ä¹ˆæ—¶å€™?"
- ç‰¹å¾: å•ä¸€äº‹å®,ç­”æ¡ˆæ˜ç¡®,é€šå¸¸åœ¨1-2ä¸ªæ–‡æ¡£ä¸­
- æ£€ç´¢ç­–ç•¥: å¿«é€Ÿæ£€ç´¢(æ–‡ä»¶ååŒ¹é… + å…³é”®è¯æœç´¢)
- é¢„è®¡é˜…è¯»æ–‡æ¡£æ•°: 1-3 ä¸ª

**Type B: æ·±åº¦åˆ†æ(analysis)**
- ç¤ºä¾‹: "é¡¹ç›®çš„ä¸»è¦é£é™©?"ã€"æŠ€æœ¯é€‰å‹çš„ä¼˜åŠ£?"ã€"ä¸ºä»€ä¹ˆé€‰æ‹©XXæ–¹æ¡ˆ?"
- ç‰¹å¾: éœ€è¦ç»¼åˆå¤šä¸ªæ–‡æ¡£,æ·±åº¦æ€è€ƒ,åˆ†æåŸå› å’Œå½±å“
- æ£€ç´¢ç­–ç•¥: æ·±åº¦æ£€ç´¢(å…¨æ–‡åˆ†æ + è·¨æ–‡æ¡£ç»¼åˆ)
- é¢„è®¡é˜…è¯»æ–‡æ¡£æ•°: 5-10 ä¸ª

**Type C: å¯¹æ¯”åˆ†æ(comparison)**
- ç¤ºä¾‹: "æ–¹æ¡ˆAå’Œæ–¹æ¡ˆBå“ªä¸ªæ›´å¥½?"ã€"Rediså’ŒMemcachedçš„åŒºåˆ«?"
- ç‰¹å¾: éœ€è¦å¯¹æ¯”å¤šä¸ªç»´åº¦,æå–å¯¹æ¯”ç‚¹
- æ£€ç´¢ç­–ç•¥: ç»“æ„åŒ–å¯¹æ¯”(æå–å¯¹æ¯”ç»´åº¦ + é€é¡¹å¯¹æ¯”)
- é¢„è®¡é˜…è¯»æ–‡æ¡£æ•°: 3-8 ä¸ª

**Type D: æ—¶é—´çº¿è¿½æº¯(timeline)**
- ç¤ºä¾‹: "é¡¹ç›®æ˜¯å¦‚ä½•æ¼”è¿›çš„?"ã€"XXåŠŸèƒ½çš„å¼€å‘å†ç¨‹?"
- ç‰¹å¾: éœ€è¦æŒ‰æ—¶é—´é¡ºåºæ•´ç†ä¿¡æ¯
- æ£€ç´¢ç­–ç•¥: æ—¶é—´åºåˆ—æ£€ç´¢(æŒ‰ä¿®æ”¹æ—¶é—´æ’åº)
- é¢„è®¡é˜…è¯»æ–‡æ¡£æ•°: 5-15 ä¸ª

#### 0.2 è¾“å‡ºä»»åŠ¡åˆ†ç±»ç»“æœ

åŸºäºç”¨æˆ·é—®é¢˜ **"$ARGUMENTS"**,æˆ‘åˆ¤æ–­è¿™æ˜¯:

```
ğŸ“‹ ä»»åŠ¡ç±»å‹: [Type A/B/C/D]
ğŸ¯ æ£€ç´¢ç­–ç•¥: [è¯´æ˜]
ğŸ“Š é¢„è®¡é˜…è¯»æ–‡æ¡£æ•°: [æ•°é‡]
â±ï¸  é¢„è®¡è€—æ—¶: [æ—¶é—´ä¼°è®¡]

ğŸ’­ åˆ¤æ–­ç†ç”±:
- [ç†ç”±1]
- [ç†ç”±2]
```

#### 0.3 ç¡®è®¤æ‰§è¡Œç­–ç•¥(é‡è¦é—®é¢˜æ‰éœ€è¦)

å¦‚æœæ˜¯ Type B/C/D(å¤æ‚é—®é¢˜),è¯¢é—®ç”¨æˆ·:

```
æˆ‘çš„ç†è§£æ˜¯ [å¤è¿°é—®é¢˜æ„å›¾]ã€‚
æˆ‘è®¡åˆ’ [æ£€ç´¢ç­–ç•¥æè¿°]ã€‚

å¦‚æœæ‚¨æƒ³è°ƒæ•´æ£€ç´¢èŒƒå›´æˆ–æ–¹å¼,è¯·å‘Šè¯‰æˆ‘ã€‚å¦åˆ™æˆ‘å°†ç»§ç»­æ‰§è¡Œã€‚
```

---

### æ­¥éª¤ 1: ç†è§£é—®é¢˜æ„å›¾

ğŸ’­ **æå–å…³é”®ä¿¡æ¯**:
- æ ¸å¿ƒå…³é”®è¯: [åˆ—è¡¨]
- é—®é¢˜ç„¦ç‚¹: [è¯´æ˜]
- æœŸæœ›ç­”æ¡ˆå½¢å¼: [äº‹å®/åˆ†æ/å¯¹æ¯”/æ—¶é—´çº¿]

### æ­¥éª¤ 2: ç¬¬ä¸€è½®ç­›é€‰(åŸºäºç´¢å¼•é¢„è§ˆ)

ğŸ’­ **æˆ‘çš„æ£€ç´¢ç­–ç•¥**:
- ä¼˜å…ˆåŒ¹é…: [æ–‡ä»¶å/å†…å®¹é¢„è§ˆ/ç›®å½•/æ ‡é¢˜]
- å…³é”®è¯ç»„åˆ: [AND/OR é€»è¾‘]
- æ—¶é—´èŒƒå›´: [æœ€è¿‘30å¤©/å…¨éƒ¨/ç‰¹å®šæ—¶æœŸ]

ä» `.notebooklm/index/metadata.json` è¯»å–ç´¢å¼•,åˆ©ç”¨é¢„è§ˆæ•°æ®å¿«é€Ÿç­›é€‰ç›¸å…³æ–‡æ¡£:

```python
import json
import re
from pathlib import Path

# è¯»å–ç´¢å¼•
with open('.notebooklm/index/metadata.json', 'r', encoding='utf-8') as f:
    index_data = json.load(f)

files = index_data['index']
query = "$ARGUMENTS"  # ç”¨æˆ·é—®é¢˜

# æå–é—®é¢˜å…³é”®è¯
def extract_keywords(text):
    """ç®€å•çš„å…³é”®è¯æå–"""
    # ç§»é™¤æ ‡ç‚¹å’Œåœç”¨è¯
    stopwords = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'åŠ', 'ç­‰', 'äº†', 'å—', 'å‘¢', 'ä»€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ'}
    words = re.findall(r'[\u4e00-\u9fa5a-zA-Z0-9]+', text)
    return [w for w in words if w not in stopwords and len(w) > 1]

query_keywords = extract_keywords(query)
print(f"ğŸ” é—®é¢˜å…³é”®è¯: {', '.join(query_keywords)}")

# è®¡ç®—ç›¸å…³åº¦å¾—åˆ†
def calculate_relevance(file, keywords):
    """åŸºäºé¢„è§ˆæ•°æ®è®¡ç®—ç›¸å…³åº¦"""
    score = 0
    reasons = []

    # æ–‡ä»¶ååŒ¹é…(æƒé‡ 2)
    for kw in keywords:
        if kw.lower() in file['name'].lower():
            score += 2
            reasons.append(f"æ–‡ä»¶ååŒ…å«'{kw}'")

    # é¢„è§ˆå†…å®¹åŒ¹é…(æƒé‡ 3)
    preview_text = file.get('preview', '')
    for kw in keywords:
        if kw in preview_text:
            count = preview_text.count(kw)
            score += min(count * 3, 9)  # å•ä¸ªå…³é”®è¯æœ€å¤šåŠ 9åˆ†
            reasons.append(f"å†…å®¹åŒ…å«'{kw}'({count}æ¬¡)")

    # PDF ç›®å½•åŒ¹é…(æƒé‡ 2)
    if file.get('has_toc'):
        toc_text = ' '.join(file.get('toc', []))
        for kw in keywords:
            if kw in toc_text:
                score += 2
                reasons.append(f"ç›®å½•åŒ…å«'{kw}'")

    # Word æ ‡é¢˜åŒ¹é…(æƒé‡ 2)
    if file.get('headings'):
        headings_text = ' '.join([h['text'] for h in file['headings']])
        for kw in keywords:
            if kw in headings_text:
                score += 2
                reasons.append(f"æ ‡é¢˜åŒ…å«'{kw}'")

    # Excel Sheet åç§°åŒ¹é…(æƒé‡ 1)
    if file.get('sheet_names'):
        sheets_text = ' '.join(file['sheet_names'])
        for kw in keywords:
            if kw in sheets_text:
                score += 1
                reasons.append(f"SheetååŒ…å«'{kw}'")

    # æœ€æ–°æ–‡æ¡£åŠ åˆ†(æƒé‡ 1)
    import time
    days_old = (time.time() - file['modified']) / (24 * 3600)
    if days_old < 30:
        score += 1
        reasons.append("æœ€è¿‘30å¤©å†…çš„æ–‡æ¡£")

    return score, reasons

# å¯¹æ‰€æœ‰æ–‡æ¡£è®¡ç®—ç›¸å…³åº¦
scored_files = []
for file in files:
    score, reasons = calculate_relevance(file, query_keywords)
    if score > 0:  # åªä¿ç•™æœ‰ç›¸å…³æ€§çš„
        file_with_score = file.copy()
        file_with_score['relevance_score'] = score
        file_with_score['relevance_reasons'] = reasons
        scored_files.append(file_with_score)

# æŒ‰ç›¸å…³åº¦æ’åº
scored_files_sorted = sorted(scored_files, key=lambda f: f['relevance_score'], reverse=True)

# ç­›é€‰ top 10 å€™é€‰æ–‡æ¡£
top_candidates = scored_files_sorted[:10]

print(f"\nğŸ“Š åˆç­›ç»“æœ: ä» {len(files)} ä¸ªæ–‡æ¡£ä¸­ç­›é€‰å‡º {len(top_candidates)} ä¸ªå€™é€‰æ–‡æ¡£")
for i, f in enumerate(top_candidates[:5], 1):
    print(f"   {i}. {f['name']} (å¾—åˆ†: {f['relevance_score']}, åŸå› : {', '.join(f['relevance_reasons'][:2])})")
```

### æ­¥éª¤ 3: ç¬¬äºŒè½®ç²¾å‡†åŒ¹é…(æ·±åº¦è§£æ)

ğŸ’­ **ç­›é€‰ç»“æœåˆ†æ**:
```
ğŸ“Š æ‰¾åˆ° [N] ä¸ªå€™é€‰æ–‡æ¡£:
   ğŸ”´ é«˜ç›¸å…³ (å¾—åˆ† > 10): [æ•°é‡] ä¸ª
   ğŸŸ¡ ä¸­ç›¸å…³ (å¾—åˆ† 5-10): [æ•°é‡] ä¸ª
   ğŸŸ¢ ä½ç›¸å…³ (å¾—åˆ† < 5): [æ•°é‡] ä¸ª

ğŸ“– æˆ‘å°†æ·±åº¦è§£æå‰ [N] ä¸ªæ–‡æ¡£,å› ä¸º [ç†ç”±]
```

å¯¹ top 10 å€™é€‰æ–‡æ¡£è¿›è¡Œå®Œæ•´è§£æ,ä½¿ç”¨å¯¹åº”çš„ **pdf/docx/xlsx Skills**:

```python
import pdfplumber
from docx import Document
from openpyxl import load_workbook

def parse_pdf_full(pdf_path):
    """å®Œæ•´è§£æ PDF"""
    with pdfplumber.open(pdf_path) as pdf:
        full_text = ""
        for page in pdf.pages:
            full_text += page.extract_text() or ""
        return full_text

def parse_docx_full(docx_path):
    """å®Œæ•´è§£æ Word"""
    doc = Document(docx_path)
    full_text = "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
    return full_text

def parse_xlsx_full(xlsx_path):
    """å®Œæ•´è§£æ Excel(ä¸»è¦ sheet)"""
    wb = load_workbook(xlsx_path, read_only=True, data_only=True)
    full_text = ""

    for sheet_name in wb.sheetnames[:3]:  # åªè§£æå‰3ä¸ªsheet
        ws = wb[sheet_name]
        full_text += f"\n=== Sheet: {sheet_name} ===\n"

        for row in ws.iter_rows(values_only=True, max_row=100):  # æ¯ä¸ªsheetæœ€å¤š100è¡Œ
            row_text = '\t'.join([str(cell) if cell else '' for cell in row])
            full_text += row_text + "\n"

    wb.close()
    return full_text

# å¯¹å€™é€‰æ–‡æ¡£è¿›è¡Œå®Œæ•´è§£æ
parsed_docs = []
for file in top_candidates:
    try:
        if file['extension'] == '.pdf':
            content = parse_pdf_full(file['path'])
        elif file['extension'] in ['.docx', '.doc']:
            content = parse_docx_full(file['path'])
        elif file['extension'] in ['.xlsx', '.xls']:
            content = parse_xlsx_full(file['path'])
        else:
            content = file.get('preview', '')

        parsed_docs.append({
            "file": file,
            "content": content[:50000]  # é™åˆ¶é•¿åº¦,é¿å…è¶…token
        })

        print(f"âœ… å·²è§£æ: {file['name']} ({len(content)} å­—ç¬¦)")

    except Exception as e:
        print(f"âš ï¸  è§£æå¤±è´¥: {file['name']} - {e}")
        continue
```

### 4. ä½¿ç”¨ Context Builder Skill æ„å»ºä¸Šä¸‹æ–‡

ä½¿ç”¨ Skill tool è°ƒç”¨ context-builder:

```python
# è°ƒç”¨ context-builder skill æ„å»ºä¸Šä¸‹æ–‡
# æ³¨æ„:è¿™é‡Œåº”è¯¥ä½¿ç”¨ Skill tool,è€Œä¸æ˜¯ç›´æ¥è°ƒç”¨ä»£ç 

# ä¼ªä»£ç ç¤ºæ„:
# context = Skill("notebooklm-assistant:context-builder", {
#     "documents": parsed_docs,
#     "query": query,
#     "max_tokens": 10000
# })

# æ‰‹åŠ¨å®ç°ç®€åŒ–ç‰ˆä¸Šä¸‹æ–‡æ„å»º:
def build_context(docs, query, max_tokens=10000):
    """ç®€åŒ–çš„ä¸Šä¸‹æ–‡æ„å»º"""
    context_parts = []
    total_tokens = 0

    for doc in docs:
        # æå–åŒ…å«å…³é”®è¯çš„æ®µè½
        content = doc['content']
        relevant_paragraphs = []

        for paragraph in content.split('\n'):
            if any(kw in paragraph for kw in query_keywords):
                relevant_paragraphs.append(paragraph)

        # æ„å»ºæ–‡æ¡£ä¸Šä¸‹æ–‡
        if relevant_paragraphs:
            doc_context = f"\n## æ–‡æ¡£: {doc['file']['name']}\n"
            doc_context += '\n'.join(relevant_paragraphs[:5])  # æœ€å¤š5æ®µ

            # ä¼°ç®—token(ç²—ç•¥:1å­—ç¬¦â‰ˆ0.5token)
            estimated_tokens = len(doc_context) // 2
            if total_tokens + estimated_tokens < max_tokens:
                context_parts.append(doc_context)
                total_tokens += estimated_tokens

    return '\n'.join(context_parts)

context = build_context(parsed_docs, query)
print(f"\nğŸ“ å·²æ„å»ºä¸Šä¸‹æ–‡: {len(context)} å­—ç¬¦ (çº¦ {len(context)//2} tokens)")
```

### æ­¥éª¤ 5: ç”Ÿæˆç­”æ¡ˆ

ğŸ’­ **ç­”æ¡ˆæ„å»ºç­–ç•¥**:
```
ğŸ“ æˆ‘å°†åŸºäºä»¥ä¸‹æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ:
1. [æ–‡æ¡£1] - æä¾› [ä¿¡æ¯ç±»å‹]
2. [æ–‡æ¡£2] - æä¾› [ä¿¡æ¯ç±»å‹]
3. [æ–‡æ¡£3] - æä¾› [ä¿¡æ¯ç±»å‹]

âš ï¸  å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰æ˜ç¡®ç­”æ¡ˆ,æˆ‘å°†:
   - æ˜ç¡®å‘ŠçŸ¥"æ–‡æ¡£åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
   - ä¸ç¼–é€ æˆ–çŒœæµ‹
   - å»ºè®®ç”¨æˆ·è¡¥å……æ–‡æ¡£æˆ–è°ƒæ•´é—®é¢˜
```

åŸºäºæ„å»ºçš„ä¸Šä¸‹æ–‡,Claude ç»¼åˆåˆ†æå¹¶å›ç­”é—®é¢˜:

**æ ¸å¿ƒåŸåˆ™(ç»ä¸å¦¥å)**:
- âŒ **ç»ä¸ç¼–é€ æ•°æ®æˆ–ä¿¡æ¯**
- âŒ **ç»ä¸ä½¿ç”¨æ¨¡å‹è®­ç»ƒæ•°æ®ä¸­çš„çŸ¥è¯†(é™¤éæ˜ç¡®æ ‡æ³¨"æ¥è‡ªAIçŸ¥è¯†åº“")**
- âœ… **ç›´æ¥å¼•ç”¨åŸæ–‡(å¸¦æ–‡ä»¶åå’Œé¡µç /ä½ç½®)**
- âœ… **ç»¼åˆå¤šä¸ªæ¥æºæ—¶,åˆ—å‡ºæ‰€æœ‰æ¥æº**
- âœ… **çªå‡ºé‡ç‚¹,å®¢è§‚é™ˆè¿°**
- âœ… **ä¸ç¡®å®šæ—¶æ˜ç¡®æ ‡æ³¨"éœ€è¦äººå·¥ç¡®è®¤"æˆ–"æ–‡æ¡£åº“ä¸­æœªæ‰¾åˆ°"**
- âœ… **å¤šä¸ªæ¥æºæœ‰çŸ›ç›¾æ—¶,åˆ—å‡ºæ‰€æœ‰è§‚ç‚¹,ä¸æ“…è‡ªåˆ¤æ–­**

å‚è€ƒä¸Šä¸‹æ–‡,ç”Ÿæˆç»“æ„åŒ–ç­”æ¡ˆ:

```markdown
## ğŸ¤” æ‚¨çš„é—®é¢˜
[å¤è¿°ç”¨æˆ·é—®é¢˜]

## âœ… ç­”æ¡ˆ
[åŸºäºæ–‡æ¡£çš„å›ç­”,æ®µè½å½¢å¼]

æ ¹æ®æ–‡æ¡£åˆ†æ,[æ ¸å¿ƒç­”æ¡ˆ]...

å…·ä½“æ¥è¯´:
1. [è¦ç‚¹1]
   > "[åŸæ–‡å¼•ç”¨]"
   > ğŸ“„ æ¥æº: æ–‡æ¡£å.pdf

2. [è¦ç‚¹2]
   > "[åŸæ–‡å¼•ç”¨]"
   > ğŸ“„ æ¥æº: æ–‡æ¡£å.docx

### ğŸ“Š å…³é”®æ•°æ®(å¦‚æœæœ‰)
| æŒ‡æ ‡ | æ•°å€¼ | æ¥æº |
|------|------|------|
| XX | YY | æ–‡æ¡£A |

### ğŸ“„ ä¿¡æ¯æ¥æº
1. [æ–‡æ¡£å1](è·¯å¾„) - ç›¸å…³åº¦: 95%
2. [æ–‡æ¡£å2](è·¯å¾„) - ç›¸å…³åº¦: 87%
3. ...

## ğŸ’¡ æ‚¨å¯èƒ½è¿˜æƒ³äº†è§£
- [ç›¸å…³é—®é¢˜1]
- [ç›¸å…³é—®é¢˜2]
- [ç›¸å…³é—®é¢˜3]

## ğŸ” æ·±å…¥æ¢ç´¢
- æ·±åº¦ç ”ç©¶: `/notebook-research [ä¸»é¢˜]`
- ç”ŸæˆæŠ¥å‘Š: `/notebook-report analysis "[ä¸»é¢˜]"`
```

### 6. æ™ºèƒ½ç¼“å­˜(å¯é€‰ä¼˜åŒ–)

å°†è§£æç»“æœç¼“å­˜åˆ° `.notebooklm/cache/` é¿å…é‡å¤è§£æ:

```python
import hashlib
from pathlib import Path

# ç¼“å­˜ç›®å½•
cache_dir = Path('.notebooklm/cache')
cache_dir.mkdir(parents=True, exist_ok=True)

# ç”Ÿæˆæ–‡ä»¶å“ˆå¸Œä½œä¸ºç¼“å­˜key
def get_file_hash(file_path, modified_time):
    """åŸºäºè·¯å¾„å’Œä¿®æ”¹æ—¶é—´ç”Ÿæˆå“ˆå¸Œ"""
    key = f"{file_path}_{modified_time}"
    return hashlib.md5(key.encode()).hexdigest()

# ä¿å­˜ç¼“å­˜
for doc in parsed_docs:
    file = doc['file']
    cache_key = get_file_hash(file['path'], file['modified'])
    cache_file = cache_dir / f"{cache_key}.json"

    with open(cache_file, 'w', encoding='utf-8') as f:
        json.dump({
            "file_path": file['path'],
            "file_name": file['name'],
            "modified": file['modified'],
            "content": doc['content']
        }, f, ensure_ascii=False)

print(f"ğŸ’¾ å·²ç¼“å­˜ {len(parsed_docs)} ä¸ªæ–‡æ¡£åˆ° .notebooklm/cache/")
```

---

## æ™ºèƒ½æ£€ç´¢åŸç†(ä¼˜åŒ–å)

### ç¬¬ä¸€è½®: åŸºäºç´¢å¼•é¢„è§ˆçš„å¿«é€Ÿç­›é€‰
- ä» metadata.json è¯»å–æ‰€æœ‰æ–‡æ¡£çš„é¢„è§ˆæ•°æ®
- åŒ¹é…æ–‡ä»¶åã€é¢„è§ˆå†…å®¹ã€ç›®å½•ã€æ ‡é¢˜ã€Sheetåç§°
- è®¡ç®—ç»¼åˆç›¸å…³åº¦å¾—åˆ†
- **ä¼˜åŠ¿**: æ— éœ€æ‰“å¼€æ–‡ä»¶,æå¿«é€Ÿ(æ¯«ç§’çº§)

### ç¬¬äºŒè½®: æ·±åº¦è§£æ top å€™é€‰æ–‡æ¡£
- åªå¯¹ç›¸å…³åº¦æœ€é«˜çš„ 10 ä¸ªæ–‡æ¡£è¿›è¡Œå®Œæ•´è§£æ
- ä½¿ç”¨å¯¹åº”çš„ pdf/docx/xlsx Skills
- æå–åŒ…å«å…³é”®è¯çš„æ®µè½
- **ä¼˜åŠ¿**: Token æ¶ˆè€—å¯æ§,å‡†ç¡®åº¦é«˜

### ç¬¬ä¸‰è½®: ä¸Šä¸‹æ–‡æ„å»ºå’Œç­”æ¡ˆç”Ÿæˆ
- ä½¿ç”¨ context-builder skill æ•´åˆä¿¡æ¯
- Claude åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ
- å¼•ç”¨æ¥æº,ä¿è¯å¯è¿½æº¯

---

## æ€§èƒ½ä¼˜åŒ–

- **ä¸¤è½®æ£€ç´¢**: å…ˆç´¢å¼•ç­›é€‰(å¿«),å†æ·±åº¦è§£æ(å‡†)
- **Token èŠ‚çœ**: åªè¯»å–ç›¸å…³æ–‡æ¡£,é¿å…å…¨é‡åŠ è½½
- **æ™ºèƒ½ç¼“å­˜**: å·²è§£ææ–‡æ¡£ç¼“å­˜å¤ç”¨
- **è¶…æ—¶ä¿æŠ¤**: å•ä¸ªæ–‡æ¡£è§£æè¶…æ—¶è·³è¿‡

---

## ç¤ºä¾‹

**ç”¨æˆ·**: `/notebook-ask é¡¹ç›®çš„ä¸»è¦é£é™©æ˜¯ä»€ä¹ˆ?`

**è¾“å‡º**:
```
ğŸ” é—®é¢˜å…³é”®è¯: é¡¹ç›®, ä¸»è¦, é£é™©

ğŸ“Š åˆç­›ç»“æœ: ä» 100 ä¸ªæ–‡æ¡£ä¸­ç­›é€‰å‡º 8 ä¸ªå€™é€‰æ–‡æ¡£
   1. risk-assessment.pdf (å¾—åˆ†: 15, åŸå› : æ–‡ä»¶ååŒ…å«'é£é™©', å†…å®¹åŒ…å«'é£é™©'(5æ¬¡))
   2. project-plan.docx (å¾—åˆ†: 8, åŸå› : å†…å®¹åŒ…å«'é¡¹ç›®'(3æ¬¡), å†…å®¹åŒ…å«'é£é™©'(2æ¬¡))
   3. meeting-notes-2024.txt (å¾—åˆ†: 5, åŸå› : å†…å®¹åŒ…å«'é£é™©'(1æ¬¡), æœ€è¿‘30å¤©å†…çš„æ–‡æ¡£)
   ...

âœ… å·²è§£æ: risk-assessment.pdf (15234 å­—ç¬¦)
âœ… å·²è§£æ: project-plan.docx (8567 å­—ç¬¦)
âœ… å·²è§£æ: meeting-notes-2024.txt (3421 å­—ç¬¦)

ğŸ“ å·²æ„å»ºä¸Šä¸‹æ–‡: 5680 å­—ç¬¦ (çº¦ 2840 tokens)

---

## ğŸ¤” æ‚¨çš„é—®é¢˜
é¡¹ç›®çš„ä¸»è¦é£é™©æ˜¯ä»€ä¹ˆ?

## âœ… ç­”æ¡ˆ
æ ¹æ®æ–‡æ¡£åˆ†æ,é¡¹ç›®é¢ä¸´ä¸‰å¤§ä¸»è¦é£é™©:

1. **é¢„ç®—è¶…æ”¯é£é™©**(é«˜ä¼˜å…ˆçº§)
   > "å½“å‰é¢„ç®—å‚¨å¤‡ä»… 5%,ä½äºè¡Œä¸šæ ‡å‡† 10-15%"
   > ğŸ“„ æ¥æº: risk-assessment.pdf

2. **è¿›åº¦å»¶æœŸé£é™©**(ä¸­ä¼˜å…ˆçº§)
   > "å…³é”®è·¯å¾„ä¸Šçš„æ¨¡å— B ä¾èµ–å¤–éƒ¨ä¾›åº”å•†,äº¤ä»˜å­˜åœ¨ä¸ç¡®å®šæ€§"
   > ğŸ“„ æ¥æº: project-plan.docx

3. **æŠ€æœ¯å€ºåŠ¡é£é™©**(ä¸­ä¼˜å…ˆçº§)
   > "å›¢é˜Ÿåœ¨ä¼šè®®ä¸­æåˆ°ç°æœ‰æ¶æ„éš¾ä»¥æ‰©å±•"
   > ğŸ“„ æ¥æº: meeting-notes-2024.txt

### ğŸ“Š é£é™©è¯„ä¼°æ±‡æ€»
| é£é™©ç±»å‹ | ä¼˜å…ˆçº§ | æåŠé¢‘ç‡ | ç¼“è§£æªæ–½ |
|---------|--------|---------|---------|
| é¢„ç®—è¶…æ”¯ | é«˜ | 3æ¬¡ | å¢åŠ å‚¨å¤‡é‡‘ |
| è¿›åº¦å»¶æœŸ | ä¸­ | 2æ¬¡ | å¯»æ‰¾å¤‡é€‰ä¾›åº”å•† |
| æŠ€æœ¯å€ºåŠ¡ | ä¸­ | 1æ¬¡ | æ¶æ„é‡æ„è®¡åˆ’ |

### ğŸ“„ ä¿¡æ¯æ¥æº
1. [risk-assessment.pdf](path) - ç›¸å…³åº¦: 98%
2. [project-plan.docx](path) - ç›¸å…³åº¦: 85%
3. [meeting-notes-2024.txt](path) - ç›¸å…³åº¦: 72%

## ğŸ’¡ æ‚¨å¯èƒ½è¿˜æƒ³äº†è§£
- è¿™äº›é£é™©çš„å…·ä½“ç¼“è§£æªæ–½æ˜¯ä»€ä¹ˆ?
- å†å²é¡¹ç›®ä¸­ç±»ä¼¼é£é™©æ˜¯å¦‚ä½•è§£å†³çš„?
- æ˜¯å¦æœ‰åº”æ€¥é¢„æ¡ˆæ–‡æ¡£?

## ğŸ” æ·±å…¥æ¢ç´¢
- æ·±åº¦ç ”ç©¶: `/notebook-research é¡¹ç›®é£é™©ç®¡ç†`
- ç”ŸæˆæŠ¥å‘Š: `/notebook-report analysis "é¡¹ç›®é£é™©è¯„ä¼°æŠ¥å‘Š"`
```

---

## æ³¨æ„äº‹é¡¹

- âœ… æ‰€æœ‰ç­”æ¡ˆåŸºäºæ–‡æ¡£,ä¸ç¼–é€ ä¿¡æ¯
- âœ… ä¸ç¡®å®šæ—¶æ˜ç¡®æ ‡æ³¨"éœ€è¦äººå·¥ç¡®è®¤"
- âœ… å¤šä¸ªæ¥æºæœ‰çŸ›ç›¾æ—¶,åˆ—å‡ºæ‰€æœ‰è§‚ç‚¹
- âœ… é‡è¦æ•°æ®å¿…é¡»æ ‡æ³¨æ¥æº
- âœ… åˆ©ç”¨é¢„è§ˆç´¢å¼•å¤§å¹…æå‡æ£€ç´¢é€Ÿåº¦
- âœ… åªæ·±åº¦è§£æé«˜ç›¸å…³æ–‡æ¡£,èŠ‚çœ Token
